



Rule following paradoxes?
Major responses: Error theory, Reductionist dispositionalism, anti-reductionist "the paradox is an illussion".
The death of pragmatics?



In the movie "blah" the aliens have a special language with a ring. While we will probably not utilize the language as a way of "time traveling" through our 4D worm, we can still end up having very new ways of communicating in the future.

For instance, in Kripke's paradox, blah blah explanation. 

Here we can see that pragmatics play a big role in determining whether or not quus means 42+64=5 at some point. But with mathematical codes as a new language of the future, that may no longer be a thing. 

While it may be the end of pragmatics, and that a fully semantic language can be utilized instead between agents using mathematical protocols.

LLM's however, are decent at picking up pragmatics as of 2024-03-22. For instance, if you write "wow I truly adore that I have to endure this job every day!" I get this:

Me: "wow I truly adore that I have to endure this job every day!" Using your knowledge of pragmatics, is that a sarcastic sentence or not?


ChatGPT: Yes, based on the typical usage of language and the context provided, it's likely that the sentence is being used sarcastically. The phrase "I truly adore that I have to endure this job every day!" suggests a level of frustration or dissatisfaction with the job rather than genuine admiration. In everyday conversation, sarcasm often involves saying the opposite of what one truly means, and in this case, the speaker is expressing discontent with their job despite using positive language."

So pragmatics may not die. However as syntactic symbols such as /s is used on the internet as an [tone indicator] to indicate sarcasm, there may be no need for pragmatics if even syntax can handle it. In either case, negating a proposition is pretty much the same thing.

$p, /s\leftrightarrow \neg p$ 



# Natural Language processing

Predictive modeling, markov chains



# Transformer Architecture



[2304.10557](https://arxiv.org/pdf/2304.10557)
[web.stanford.edu/~jurafsky/slp3/9.pdf](https://web.stanford.edu/~jurafsky/slp3/9.pdf)

Self-attention/multi-head attention

> Attention can be thought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans.

![[Pasted image 20241206161040.png]]

Each block is a multilayer-network. It is a multi-head attention layer, feedforward networks and layer normalization steps. 

"At the center are columns of transformer blocks. Each block is a multilayer network (a multi-head attention layer, feedforward networks and layer normalization steps) that maps an input vector xi in column i (corresponding to input token i) to an output vector hi. The set of n blocks maps an entire context window of input vectors (x1 xn) to a window of output vectors (h1 hn) of the same length. A column might contain from 12 to 96 or more stacked blocks."

"The columnofblocks is preceded by the input encoding component, which pro cesses an input token (like the word thanks) into a contextual vector representation, using an embedding matrix E and a mechanism for encoding token position. Each column is followed by a language modeling head, which takes the embedding out put by the final transformer block, passes it through an unembedding matrix U and a softmax over the vocabulary to generate a single token for that column."





## Big Data
Scrapping big data from the internet, enormous data-bases to train on, scaling laws


[Scientific Research and Big Data (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/science-big-data/)




## Softmax

[Softmax function - Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)
> The softmax function is often used as the last [activation function](https://en.wikipedia.org/wiki/Activation_function "Activation function") of a [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network "Artificial neural network") to normalize the output of a network to a [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution "Probability distribution") over predicted output classes.

# Large Language Models







Old blog post idea:

**What AI can teach us about language

  

How do we learn language? Do we learn it through repetition with rewards and punishment, or do we have innate tools for building grammar? The first account can be called the Behaviourist school of thought, mostly founded on the theories of psychologist B.F Skinner. The second can be called Chomskyian, named after linguist Noam Chomsky who proposed a theory of universal grammar and the minimalist program. This question might be answered by which AI technology will solve the Turing test.

  

The Turing Test, also known as the imitation game, is a scenario involving two people and one bot. The bot and one of the humans chat with the second human, and if the second human cannot distinguish between who is the bot and who is the human, the bot passes the Turing test.

  

People started to attempt creating these kinds of bots, but at first they were rather basic with a bunch of “if then” statements but has since expanded into full fledged semantic-parsing bots such as Alexa by Amazon or Siri by Apple. There have been serious developments of conversational AI in the last few years, especially in the field of natural language processing since AI as a whole has exploded in popularity, especially as computers have gotten more powerful and huge datasets have been gathered.

  

There are two major types of natural language processing: Symbolic and Neural. Symbolic used to be dominant, but has been superseded by the neural type. The symbolic can be explained as a system where the bot is programmed by the programmers exactly how it should parse the grammar and exactly how it should respond in response to input from the user. It is the original approach of multiple "if, then" statements, for example, if the user inputs "who was the first person to walk on the moon?" the computer would parse it using if else statements and after pre-established grammatical rules to then build a reply to send to the user using the same rules. The problem with this approach is that the bot is really dumb, and only workes in narrow confines that the programmers set up. 

  

The statistical, neural network approach to conversational AI has been developed rapidly in recent years. It uses large sample sizes of data to train on to minimise a loss function via a backpropagation algorithm that updates the neural net depending on rewards and punishments. This is atleast what reinforcement learning is about, which is the technology most modern natural language processing uses. [SOURCE]

  

Now this lends into an interesting philosophical question that goes straight into the behaviourist vs chomskyan approaches in the philosophy of linguistics. If an AI that passes the Turing test learns all the grammar based on a reinforcement learning algorithm with no pre-built symbolic based approaches, would this support the behaviourists side? Or conversely, if it would be necessary to have a hybrid approach as discussed earlier for the AI to pass the Turing test, would this support the chomskyan side of the existence of universal grammar? What about the necessity of inborn/innate knowledge and tabula rasa theory? 

  
  
  
  

Noam chomsky in the book “The minimalist programme” he lays out the following foundations for the project of universal grammar: 

  

“The language faculty has an initial state, genetically determined; in the normal course of development it passes through a series of states in early childhood, reaching a relatively stable steady state that undergoes little subsequent change, apart from the lexicon. To a good first approximation, the initial state appears to be uniform for the species. Adapting traditional terms to a special usage, we call the theory of the state attained its grammar and the theory of the initial state Universal Grammar (UG)”

  

This bears some resemblance to the idea of symbolic manipulation discussed earlier. If symbolic manipulation has not worked for AI, how come it works for humans? 

  
  
  
  
  

To begin answering this question, we first need a theory of what language really is. If language is constructed a certain way, it could indicate both how humans learn/use language and how AI uses it. When I'm writing in a language, I'm performing a cognitive process that turns thought into words. A cognitive process is also a conscious process, in the sense that I'm actively attending to the process, and I'm experiencing it in my inner voice. This leads to my first important thesis: language is a conscious process. Language without consciousness is contradictory, since language itself is a conscious process. 

  

Since I also take in information from my surroundings to perform cognitive tasks, i.e I cannot do cognitive tasks without stimulus of some kind. This viewpoint is known as extended cognition and if true, would mean that bots also experience stimulus of some kind. This is because it is necessary for them to experience something to even be able to cognize and perform an output action. This is true even for computers without any sensors, since it’s all some kind of sensory information at the end of the day. 

  

Some might use the Chinese room argument to claim that the computer is not understanding anything, hence it’s only a rule-performing machine, and is hence not conscious. The point is however that the whole room understands it, even if the constituent parts don't. And that implies a cognitive process, and that is a conscious process. 

  
  

Something about the symbol grounding problem. Do objects in Frege’s system even need to exist? Yes.

  
  

One of the problems with the turing test is that it tests AI intelligence on human terms. It could be very intelligent, maybe even more intelligent than the smartest humans, and could still fail the Turing test. Not because it wants to (because it can deceive) but because intelligence is probably more than just human convention. 

  
  
  

An Artificial General Intelligence that performs roughly human standards of intelligence with conscious experience is what is necessary to complete the Turing test. Keep in mind that an AGI is not a hyperintelligence and wouldn’t necessarily fool the human into thinking it was dumber than it really was. 

  
  

Philosophical zombies. Input output. E.t.c.

If a human, with given inputs, react with a certain output. It is conceivable that a machine can be built to perform the same action, given the same input. 

  

  
  
  
  

When you turn language into statistics you can map the probabilities of various phrases. A meta mesmeriser will look over how to best match human language. 

  

AI deception, language as a tool for AGI to use to communicate with us puny humans.

  
  
  
  

Sources:

  

Cognition as Embodied Morphological Computation, Gordana Dodig-Crnkovic

  

Where Intelligence Lies: Externalist and Sociolinguistic Perspectives on the Turing Test and AI, Shlomo Danziger

  

Where Intelligence Lies: Externalist and Sociolinguistic Perspectives on the Turing Test and AI Shlomo Danziger

  
  

[https://en.wikipedia.org/wiki/Symbol_grounding_problem](https://en.wikipedia.org/wiki/Symbol_grounding_problem) 

  

[http://www.scu.edu.tw/philos/98class/Peng/05.pdf](http://www.scu.edu.tw/philos/98class/Peng/05.pdf) 

  

[https://arxiv.org/pdf/2004.05631.pdf](https://arxiv.org/pdf/2004.05631.pdf) 

  

Symbol grounding problem solved

[https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.3463&rep=rep1&type=pdf](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.3463&rep=rep1&type=pdf) 

  

[https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control](https://www.lesswrong.com/posts/ZDZmopKquzHYPRNxq/selection-vs-control) 

  

[https://www.lesswrong.com/posts/XWPJfgBymBbL3jdFd/an-58-mesa-optimization-what-it-is-and-why-we-should-care](https://www.lesswrong.com/posts/XWPJfgBymBbL3jdFd/an-58-mesa-optimization-what-it-is-and-why-we-should-care)**



[*A Philosophical Introduction to Language Models.pdf](file:///D:/Files/University%20Backup/Phil%20of%20AI/A%20Philosophical%20Introduction%20to%20Language%20Models.pdf)

> In this opinionated review paper, we argue that LLMs are more than mere Blockheads; but this skeptical interpretation of LLMs serves as a useful foil to develop a subtler view. While LLMs can simply regurgitate large sections of their prompt or training sets, they are also capable of flexibly blending patterns from their training data to produce genuinely novel outputs. Many empiricist philosophers have defended the idea that sufficiently flexible copying of abstract patterns from previous experience could form the basis of not only intelligence, but full-blown creativity and rational decision-making (Baier 2002, Hume 1978, Buckner 2023); and more scientific research has emphasized that the kind of flexible generalization that can be achieved by interpolating vectors in the semantic spaces acquired by these models may explain why these systems often appear more efficient, resilient, and capable than systems based on rules and symbols (Smolensky 1988, Smolensky et al. 2022a). A useful framework for exploring the philosophical significance of such LLMs, then, might be to treat the worry that they are merely unintelligent, inefficient Blockheads as a null hypothesis, and survey the empirical evidence that can be mustered to refute it.5


>going back to recurrent neural networks (RNNs) and their variants, such as long short-term memory (LSTM) (Hochreiter & Schmidhuber 1997) and the gated recurrent unit (GRU) (Cho et al. 2014).




# Large language models (probably a copy)


# Tokenization

Using tokenization, one can take a text input and convert that text-input into 

Tokens are not necessarily the same as words

"I love tiktoken!"
"I", "love", "t", "ik", "token", "!"


The tokens are then arranged into a vector space. During the training process of the language model, this vector space gets updated based on new data. 

LLMs are not supervised learning, but uses self-prompting




# Self Attention

Allows for picking out the most important words in a text


> [Sebastian Raschka - Build a Large Language Model (From Scratch) (2024, Manning Publications Co.) - libgen.li.pdf](file:///C:/Users/Katte/Documents/Academic/AI/Sebastian%20Raschka%20-%20Build%20a%20Large%20Language%20Model%20(From%20Scratch)%20(2024,%20Manning%20Publications%20Co.)%20-%20libgen.li.pdf)
> Before the advent of transformers, recurrent neural networks (RNNs) were the most popular encoder-decoder architecture for language translation.

> The big issue and limitation of encoder-decoder RNNs is that the RNN can't directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.

> One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder, as illustrated in Figure 3.4 in the previous section. Hence, researchers developed the so-called Bahdanau attention mechanism for RNNs in 2014 (named after the first author of the respective paper), which modifies the encoder-decoder RNN such that the decoder can selectively access different parts of the input sequence at each decoding step as illustrated in Figure 3.5.

Self attention mechanism in LLMs is inspired by the Bahdanau attnetion mechanism for RNNs.

![[Pasted image 20250112130915.png]]

So I presume pronouns like "it" can refer importantly in the past to previous token that it might be referring to. Ambiguity could then in some sense be the the mechanism giving similar values to two different say, proper names to the "it" token? Something like that?

![[Pasted image 20250112131743.png]]

Each token x gets its own context vector based on the attention weights of each token related to x.


Use dot product together with a "query" token (unsure what that is) and craete a new vector out of the dot products from all other words in the input sequence

![[Pasted image 20250112133642.png]]

Normalized to sum up to 1 as a convention, mathematical simplicity and stability

![[Pasted image 20250112133700.png]]

Softmax is used to normalization!!


## Causal attention

Only past and current inputs count

## Multi-Head attention

