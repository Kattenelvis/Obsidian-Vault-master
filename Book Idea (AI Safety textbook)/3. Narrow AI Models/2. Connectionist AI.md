

## Neural Networks

### Feed-Forward

Definition 1: The input layer
Definition 1: Hidden layers
Definition 1: The Output layer

The connections goes from layer to layer, forming a so called feed-forward neural network. It can be represented as a mathematical graph, which can also have a vector representation.

Each node $n$ has an activation value $a(n)$, and when some function take the inputs from the edges $e_1,\dots e_n$ such that $f(e_1,\dots e_n)> a(n)$ then $n$ activates and sends its signal forward.

Usually such an $f$ is utilizes the sigmoid function or similar functions which map it to between 0 and 1. Another type of function is used more these days forgot the name but it's linear rather than non-linear. 

Deep Neural Networks have many hidden layers


### Recurrent neural networks


One theory of consciousness, namely recurrent theory (TODO: What was the name again?), posits consciousness as recurrent neural networks.

Feed forward neural networks, which are the most common, don't have that. They don't send the signal recurrently. 

[Introduction to Recurrent Neural Networks - GeeksforGeeks](https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/)

Example of physical implementation of such a system:
[The Neuroscience of Consciousness (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/consciousness-neuroscience/#RecuProcTheo)1


> _****Recurrent Neural Networks (RNNs)****_Â were introduced in the 1980s by researchersÂ __David Rumelhart, Geoffrey Hinton, and Ronald J. Williams__


> Lamme identifies four stages of normal visual processing:

> - Stage 1: Superficial feedforward processing: visual signals are processed locally within the visual system.
> - Stage 2: Deep feedforward processing: visual signals have travelled further forward in the processing hierarchy where they can influence action.
> - Stage 3: Superficial recurrent processing: information has traveled back into earlier visual areas, leading to local, recurrent processing.
> - Stage 4: Widespread recurrent processing: information activates widespread areas (and as such is consistent with global workspace access).

> Lamme holds that recurrent processing in Stage 3 is necessary and sufficient for consciousness.

> Thus, recurrent processing theory affirms phenomenal consciousness without access by the global neuronal workspace. In that sense, it is an overflow theory


> It is worth reemphasizing the empirical challenge in testing whether access is necessary for phenomenal consciousness




< From geeksforgeeks>
#### Advantages of Recurrent Neural Networks

- ****Sequential Memory****: RNNs retain information from previous inputs, making them ideal for time-series predictions where past data is crucial. This capability is often calledÂ ****Long Short-Term Memory****Â (LSTM).
- ****Enhanced Pixel Neighborhoods****: RNNs can be combined with convolutional layers to capture extended pixel neighborhoods, improving performance in image and video data processing.

#### Limitations of Recurrent Neural Networks (RNNs)

While RNNs excel at handling sequential data, they face two main training challenges i.e.,Â [vanishing gradient and exploding gradient problem](https://www.geeksforgeeks.org/vanishing-and-exploding-gradients-problems-in-deep-learning/):

1. ****Vanishing Gradient***: During backpropagation, gradients diminish as they pass through each time step, leading to minimal weight updates. This limits the RNNâ€™s ability to learn long-term dependencies, which is crucial for tasks like language translation.
2. ****Exploding Gradient***: Sometimes, gradients grow uncontrollably, causing excessively large weight updates that destabilize training. Gradient clipping is a common technique to manage this issue.

These challenges can hinder the performance of standard RNNs on complex, long-sequence tasks.

#### Applications of Recurrent Neural Networks

RNNs are used in various applications where data is sequential or time-based:

- [****Time-Series Prediction****](https://www.geeksforgeeks.org/time-series-forecasting-using-recurrent-neural-networks-rnn-in-tensorflow/?ref=asr3): RNNs excel in forecasting tasks, such as stock market predictions and weather forecasting.
- [****Natural Language Processing (NLP)****](https://www.geeksforgeeks.org/rnn-for-text-classifications-in-nlp/): RNNs are fundamental in NLP tasks like language modeling, sentiment analysis, and machine translation.
- ****Speech Recognition***: RNNs capture temporal patterns in speech data, aiding in speech-to-text and other audio-related applications.
- ****Image and Video Processing***: When combined with convolutional layers, RNNs help analyze video sequences, facial expressions, and gesture recognition.

< /geeksforgeeks>



## Deep Neural Networks

Are characterized by a large increase in the number of hidden layers. Instead of a couple, it can be several hundred.

> The number of layers is not the only feature of deep nets that explain their superior abilities. An emerging consensus is that many tasks that are hard to learn are characterized by the presence of â€œnuisance parametersâ€, sources of variation in input signals that are not correlated with decision success. Examples of nuisance parameters in visual categorization tasks include pose, size, and position in the visual field; examples in auditory tasks include tone, pitch, and duration. Successful systems must learn to recognize deeper similarities hiding under this variation to identify objects in images, or words in audio data.



### Convolutional neural networks

> One of the most commonly-deployed deep architecturesâ€”deep convolutional networksâ€”leverages a combination of strategies that are well-suited to overcoming nuisance variation. Golden Age nets used the same activation function for all units, and units in a layer were fully connected to units in adjacent layers. However, deep convolutional nets deploy several different activation functions, and connections to units in the next higher layer are restricted to small windows, such as a square tile of an image or a temporal snippet of a sound file.


### Interpretable and Explainable AI

With the rise of deep neural networks, and the abandonment of GOFAI, and with "object recognition" in neural networks being chimera and oddly shaped, we can only come to the conclusion that to figure out its "common-sense" psychology, such as it's belief states, we need it to be interpretable. 

[Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI](https://sci2s.ugr.es/sites/default/files/ficherosPublicaciones/2736_1-s2.0-S1566253519308103-main.pdf)

> One of the issues that hinders the establishment of common grounds is the interchangeable misuse of interpretability and explainability in the literature. There are notable differences among these concepts. To begin with, interpretability refers to a passive characteristic of a model referring to the level at which a given model makes sense for a human observer. This feature is also expressed as transparency. By contrast, explainability can be viewed as an active characteristic of a model, de- noting any action or procedure taken by a model with the intent of clarifying or detailing its internal functions


Taking the syntax of a set of nodes $n_1,...n_N$ and give it an interpretation $I$. 


Explanations [[Scientific Explanation]]

Idea: Deductive nomonological approach where some of the "natural laws" are its own thought process?



> Since explaining, as argumenting, may involve weighting, comparing or convincing an audience with logic-based formalizations of (counter) arguments [28] , explainability might convey us into the realm of cogni- tive psychology and the psychology of explanations [7] , since measuring whether something has been understood or put clearly is a hard task to be gauged objectively.


![[Pasted image 20241215135710.png]]



![[Pasted image 20241215140520.png]]
![[Pasted image 20241215140538.png]]

![[Pasted image 20241215141113.png]]
> Fig. 3. Conceptual diagram exemplifying the different levels of transparency characterizing a ML model M ð‹ , with ð‹ denoting the parameter set of the model at hand: (a) simulatability; (b) decomposability; (c) algorithmic transparency. Without loss of generality, the example focuses on the ML model as the explanation target. However, other targets for explainability may include a given example, the output classes or the dataset itself.




![[Pasted image 20241215140945.png]]

# Cognitive psychology and connectionism


[Associationist Theories of Thought (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/associationist-thought/)

[Connectionism (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/connectionism/)



Neural networks consist of an input layer, an output layer, and a set of hidden layers. Behavourists posited merely input-ouput relationships between sensory inputs and motor outputs. The cognitivist can say that we can include some computations that occur inbetween these steps by positing a set of neural connections. 




Intelligence as an emergent property of computational systems?


[Ramsey-ConnectionismEliminativismFuture-1990.pdf](file:///C:/Users/Katte/Documents/Academic/Philosophy/Ramsey-ConnectionismEliminativismFuture-1990.pdf)

Eliminativism on certain propositional attitudes, such as beliefs, desires, inferences, actions and so on can be taken to be eliminated by connectivist theories of cognition. The propositional attitudes are generally too distributed in the weights and biases of the neural network. This would mean that propositional attitudes, including their properties such as functional discreteness and propositional modularity are lost. 

With interpretability research this might be less the case however. Interpretability research is tryin to find something like propositional attitudes in neural networks. Perhaps interpretability research can help finish some kind of eliminativist project even for humans.

Of course, one persons ponens is anothers tollens. We can use these arguments by saying that since we do have propositional attitudes, then there's no way connectivist models of cognition can possibly be true. 

>Because of their obvious, though in many ways very partial, similarity to real neural architectures, it is tempting to view connectionist models as models of the implementation of psychological processes. And some connectionist model builders endorse this view quite explicitly. So viewed, however, connectionist models are not psychological or cognitive models at all, any more than a story of how cognitive processes are implemented at the quantum mechanical level is a psychological story. A very different view that connectionist model builders can and often do take is that their models are at the psychological level, not at the level of implementation.


The authors argues that it's not possible to encode propositonal modularity. Despite this, they managed to token swap GPT-2 for instance, finding the belief that "Paris is the capital of France" and being able to induce beliefs such as "Rome is the capital of France". 

## Reinforcement Learning

Pavlov famously ... dogs bell

Well we have the same thing now with AI. This kind of agent model takes action, receives feedback/reward from the environment based on those actions, and revises one's action strategy accordingly

Mathematically speaking:
$A={R, A}$
environment state-action pairs
utility goes brrr


A simple model is the multi-armed bandit
(insert that here)


Woaw!




[Probably approximately correct learning - Wikipedia](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning)
Also found in the computational complexity paper!


## Neural logic

## Statistical Learning
## Generative Adversarial Networks, Diffusion models, LLMs

