- Philosophy of Science
	- Alignment research: Pre-paradigmatic?
	- How to make studies into future AI models more scientific
	- Establishing multiple competing Lakatoshian AI research projects (and why Kuhn is somewhat wrong).








# Establishing multiple competing Lakatoshian AI research projects (and why Kuhn is somewhat wrong).


Popperian Science

Philosophy of science as rational reconstruction instead of normative behaviour. Yes this is true, we let AI research figure out how to actually learn.

Kuhnian Paradigms, they're wrong 

Lakatosh is mostly right probably.


Here's the Lakatoshian research project in AI safety


So what are the core beliefs and what are belt-beliefs of different research programes? What are beliefs we could give up and change around new data? Are there any AI safety research programs that can make novel predictions in the face of anomalies?


## How to spot a degenerating research program


We already know that many AI research programs have degenerated to the point of no return, especially most GOAI programs. As such, we will explore how to spot if AI safety programs are degenerating, and when one should "jump the ship", so to speak, instead of merely changing belt beliefs. 



# Using philosophical methodology (thought experiments) to prove points. How valid is that?


One thing that may be implicit in parts of this book and in AI blogs in general is the use of thought experiments to think about AI. This is a methodology often used in philosophy, but is used in a different way. 






# Scientific Discovery

Logic discovery in medicine 1985
"The new concepts introduced by a theory of this kind cannot, as a rule, be defined by those previously available; they are characterized, rather, by .means of a set of theoretical principles linking the new concepts to each other and to previously available concepts that serve to describe the phenomena to be explained. Thus the discovery of an explanatory theory for a given class of occurrences requires the intro duction both of new theoretical terms and of new theoretical principles. It does not seem clear at all how a computer might be programmed to discover such powerful theories. It was no doubt this kind of consideration that led Einstein to emphasize that there are no general systematic procedures which can lead the scientist from a set of data to a scientific theory, and that powerful theories are arrived at by the exercise of the scientist's free creative imagination."


So the idea seems simple enough. Have a database of hypothesises (diseases) and a list of symptoms (observables) and test. If an symptom shows up that contradicts the symptoms of some diseases, falsify that disease and keep the rest. Continue until you have only one left, alternatively if there aren't enough symptoms then you have underdetermination. In such a case, one can treat it as if it's the most common one, until more symptoms show up, so long as the side-effects aren't bad enough (not to mention the side-effects might hide the disease).

But this didn't work. 

Hypothesises cannot be tested in such a simplistic manner. Logic in the discovery of medicine (1985) have some philsohophers lamenting over why this didn't end up working in practice. Coming up with hypothesises is not an easy task, and even if the stages of falsification and verification (or perhaps, [bayesian confirmation]) can be algorithmic, there's no guarantee that such a nice list of diseases, with the necessary and sufficient symptoms that are exactly the same for all individuals, still not including hypochondria, the persons age, sex, genetics etc. nor environmental effects or mental illnesses with physical effects.  

And it gets worse. Hypothesis formation may contain irrational elements of creativity. A common story in the philosophy of science is that of the hypothesis of the carbon ring molecule where the chemist dreamt of a snake biting it's own tail[source].

[Picture of tail]

This doesn't mean it can't be replicated by sufficiently advanced AI of course, just that there is no simple algorithm the way one might've expected. 