

[0712.3329](https://arxiv.org/pdf/0712.3329)
[jagi-2019-0002](https://intapi.sciendo.com/pdf/10.2478/jagi-2019-0002)



Carnapian explication!!!!



Carnapian explication is the view that we approach definitions, not by trying to find the one true definition, nor by trying to find some intuitively correct definition that fits all edge cases. Instead we utilize a definition whereby such definition upholds certain criterions of usefulness within some theory. Typically this might include simplicity, fruitfullness, exactness and atleast partial similarity to the "intuitive" or "folk" notion. 



The risk of defining intelligence as some capability that humans, but not algorithms have, is that there would be constant goal-shifting as AI's are capable of more tasks that would no longer classify as intelligent.


$AI_{wang} =_{df}$ Open ended algorithm under resource, knowledge and time constraints. Picks algorithms to execute on problems. 


So this works for NARS models. 



$AI_{hutter} =_{df}$ Being able to achieve goals in a large range of environments


A set of environment states $\{S_1,\dots S_n\} = S$ 

We can take it that a goal state $G\in S$.

Agent maximises reward. 






![[Pasted image 20241223002123.png]]


Input 
Cognitive States
Output

$<P,C,O>$
$<P^H,C^H,O^H>$
$<P^A,C^A,O^A>$

Function AI just says it has to be similar, based on functional input output 

$P^H\approx P^A$ etc.

> A less radical position is to define AI as using computers to solve problems that are only solvable by the human mind, as suggested by Minsky (1983). This treatment will indeed separate some problems from the others, but it leads to an iconic result: as soon as a computer system is built to solve a problem successfully, the problem is no longer “only solvable by the human mind,” so does not need intelligence anymore. Consequently, “AI is whatever hasn’t been done yet” (Hofstadter, 1979; Schank, 1991), which is known as “the AI Effect” (McCorduck, 2004).






According to Wang, there's no upper limit. However for Hutter, the upper limit is AIXI. The benefit of having a maximal intelligence is that one can predict the post-singularity society by analyzing how AIXI would behave. 






Definition of RL (Stockholm 2023 AGI conference)

![[Pasted image 20241225102626.png]]

![[Pasted image 20241225102637.png]]

![[Pasted image 20241225102646.png]]

[Bellman equation - Wikipedia](https://en.wikipedia.org/wiki/Bellman_equation)


The pyramid definition 2023 AGI conference the first paper. Apparently includes both Wang and Hutter in a more grand set of definitions, and also Goerzel. 



The pyramidal definition asserts that certain capabilities or goal achievements for certain agents are possible or not. The idea is that an fully developed AGI can achieve all tasks on roughly or greater human level. Superintelligence would likely superseeded and introduce new capabilities humans have not been able to achieve yet. 








![[Pasted image 20241226205041.png]]

[Eitan Michael Azoff - Toward Human-Level Artificial Intelligence_ How Neuroscience Can Inform the Pursuit of Artificial General Intelligence or General AI-CRC Press (2024).pdf](file:///C:/Users/offic/Downloads/Eitan%20Michael%20Azoff%20-%20Toward%20Human-Level%20Artificial%20Intelligence_%20How%20Neuroscience%20Can%20Inform%20the%20Pursuit%20of%20Artificial%20General%20Intelligence%20or%20General%20AI-CRC%20Press%20(2024).pdf)

![[Pasted image 20241226205123.png]]