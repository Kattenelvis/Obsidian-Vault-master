

Predicting the future is hard
[black swans, new nostradamuses voodoo decision theories.pdf](file:///C:/Users/Katte/Documents/Academic/Philosophy/black%20swans,%20new%20nostradamuses%20voodoo%20decision%20theories.pdf)


# The Singularity
j
Hutter's singularity

**- Infinite progress in finite time  
      
    
- Postulated by Ulam and Von Neumann, popularized by Ray Kurzweil  
      
    
- Many ways it might be achieved:
    

- Brain Scans
    
- Better and Better AI systems
    
- “Awakening of the internet”  
      
    

- The main idea being that software based agents can create new, improved versions of themselves.**
**  

(a) there is something like intelligence, 

(b) there are many cognitive capacities correlated with intelligence,

(c) these capacities might explode, therefore

(d) intelligence might amplify and explode.  
  

While Hutter mostly agrees with this analysis, he does not think it tell us what a society of ultra-intelligent beings might look like.  
  

Hutter also takes it that there are no defeaters to such a singularity, such as disinclination

**

**

- Static Comp: Incomprehensible to outsiders, may only be studied using some kind of statistical mechanics/sociophysics  
      
    
- There is lots of motivation to compress information (save memory, extract regularities, and others)  
      
    
- Looks like white noise (maximal information compression does that)  
      
    
- Increasing Comp: The singularity expands outwards, outsiders must flee, but likely won’t be able to
    

**

**

- Static comp: Evolutionary pressures inside of virtual worlds  
      
    
- Creation of virtual worlds, culture, fashion  
      
    
- No need for a theory of everything, the underlying computational process is already known  
      
    
- Increasing comp: The world outside seems like it slows down, the virtual citizens wouldn’t notice anything (at uniform speed up for each agent)  
      
    
- Instead, one may spend the extra comp on more things, the intelligence of the collective may be faster with a larger population.
    

**

**

- Intelligence is not necessarily being speedy  
      
    
- You’re not necessarily more intelligent if we say that the environment, including all other humans, are moving and speaking in slow-motion  
      
    
- Your co workers talking slowly wouldn’t make you more intelligence  
      
    
- A faster environment may even increase intelligence, as rewards signals are given faster  
      
    
- So what is intelligence then?
    

**

**

- Expected performance averaged over all environments/problems the agent potentially has to deal with, with an Ockham’s razor inspired prior weight for each environment  
      
    
- Measurable by Y (number between 0 and 1)  
      
    
- Y(AIXI) = 1  
      
    
- Y(Human) = ?
    

**
AIXI is maximally intelligence

**

- Will a pure reward maximizer such as AIXI listen to and trust a teacher? Likely yes.  
      
    
- Will it take drugs (i.e. hack the reward system)? Likely no, since cumulative long-term reward would be small (death).  
      
    
- Will AIXI replicate itself or procreate? Likely yes, if AIXI believes that clones or descendants are useful for its own goals.  
      
    
- Will AIXI commit suicide? Likely yes (no), if AIXI is raised to believe in going to heaven (hell) i.e. maximal (minimal) reward forever.  
      
    
- Will sub-AIXIs self-improve? Likely yes, since this helps to increase reward.  
      
    
- Will AIXI manipulate or threaten teachers to give more reward? Likely yes.  
      
    
- Are pure reward maximizers like AIXI egoists, psychopaths, and/or killers, or will they be friendly (altruism as extended ego(t)ism)?  
      
    
- Curiosity killed the cat and maybe AIXI, or is extra reward for curiosity necessary?  
      
    
- Immortality can cause laziness, Will AIXI be lazy?  
      
    
- Can self-preservation be learned or need (parts of) it be innate.  
      
    
- How will AIXIs interact/socialize in general?  
      
    
- Which game theoretic strategies would they use against eachother?*
    

**


## Singularities, Super intelligences and Oracles

A very commonly discussed idea is that of a technological singularity[source]. Similar to our discussion in [] about how technology is super-exponential, the idea is that technological progress eventually reaches a point where technology self improves at a massive scale. Such a technology may be an self-modifying AGI that's able to reason, perform science about oneself and engineer oneself into a more intelligent version of itself over time. It is postulated by some that this would eventually lead to the creation of a superintelligence, and as such it will be able to achieve almost any goal in almost any environment. This can make it incredibly hard to control. 

However some argue that there may be roadblocks, and anything more intelligence than say Von Neumann would hit diminishing marginal return on intelligence. As such, one could for instance construct a swarm of Von-Neumann's, perhaps thousands to millions of them, to solve and optimize the more difficult goals together as a collective. 

Even if that doesn't turn out to work, we would still end up with potentially amazing outcomes, given what just one Von Neumann was able to accomplish. As such, curing cancer, and stopping global warming, solving problems of interstellar space travel, solving quantum gravity and the nature of consciousness, may not be very far off. But it may not be as dramatic and as rapid as an intelligence singularity. 

In Nick Boströms book, there are discussions about building an Oracle. Such an intelligence can answer any question.


There has been a long history of Oracles fro Llul and Leibniz to Turing:
"In his theorizing Leibniz described what he called an _ars inveniendi_, a discovering or devising method. The function of an _ars inveniendi_ is to produce hitherto unknown truths of science (Leibniz 1679 [1903: 37]; Leibniz _n.d._2 [1890: 180]; Hermes 1969). A mechanical _ars inveniendi_ would generate true statements, and with time the awaited answer to a scientific question would arrive (Leibniz 1671 [1926: 160]). Blessed with a universal (i.e., complete, and consistent) _ars inveniendi_, the user could input _any_ meaningful and unambiguous (scientific or mathematical) statement S, and the machine would eventually respond (correctly) with either “S is true” or “S is false”. As the groundbreaking developments in 1936 by Church and Turing made clear, if the _ars inveniendi_ is supposed to work by means of an effective method, then there can be no universal _ars inveniendi_—and not even an _ars inveniendi_ that is restricted to all mathematical statements, since these include statements of the form “p is provable”, or even to all purely logical statements."
https://plato.stanford.edu/entries/church-turing/ 

### Omniscience, achievable engineering goal?

This section delves more deeply into metaphysics and most readers interested in an empiricist part of the project of AI can skip this section without problem. 

In theology and philosophy of religion, there have long been discussions on divine omniscience. The idea is that God knows everything, every proposition is known by God. As God is often represented as an agent, ascribed by mental states such as "knows that" and "believes that".

Can we build it?

One obvious reason this isn't possible is due to Gödel's Incompleteness Theorem. It states that not all true statements are provable, so any sufficiently advanced AI capable of proof checking will still never for instance, prove the Continuum Hypothesis in ZFC, even though it might be true. Only something divinely omniscient would know such a proposition. Logical omniscience is thus impossible. 

$\exists p. p\leftrightarrow \neg Prov(\ulcorner p\urcorner)$

$\forall p Prov(p)\rightarrow K^{/\\} (p)$
$\neg\diamond\forall p (p\rightarrow K(p))$

Another problem is knowledge of the past and knowledge of the future. A problem often studies by theologians and philosophers is divine foreknowledge and free will. But a problem arises for advanced AI. It may not be able to predict the future entirely. It must be able to predict itself, in the now, which leads to self-referential paradoxes. 

Another problem is if natural laws are in some way indeterministic[QM source] or even if they were deterministic, they may be irretractably chaotic [chaos theory]. Even if the laws of physics are reversible i.e $f^{-1}(U_{t+1}) = U_t$ every proposition about the past may not be known due to the unknowability of $U_t$.

QM is timesymmetric and has no information loss. Black hole paradox?

So with that impossible, how about a lighter version of Oracle? Maybe we'll call it Small-Oracle. While Small-Oracle cannot solve uncomputable problems (barring it figures out hypercomputation) nor predict the future or past with perfect accuracy, maybe it will be able to solve enough problems, such as curing cancer and global warming? It is perhaps so the case. But another problem is in front of us: Maybe an oracle, even mini-oracle, is only possible by requiring it to take actions to interact with the world? Well, a superintelligence could firure out most things quickly so maybe not idk. 



