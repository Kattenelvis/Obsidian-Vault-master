Alt title: The Foundation of Computation

AI's are computational, that is to say, they are implementing computations. But what really is a *computation*? To answer that question we will get a bit technical, but will be useful for many future chapters. The next chapter will go over on how computations are *implemented*. 

One way of thinking about computation intuitively is as an input-output process. The idea is you feed something in to get something out, following strict manipulation rules. Sometimes you input nothing, sometimes you get nothing. Sometimes the process will take forever and you will never know if it will end. This seemingly simple idea has deep consequences, and has is foundational for computation. How this idea works more formally is what will be explicated in this chapter. 
# Foundations of computation

Lots of good sources here:
[The Symbol Grounding Problem](https://arxiv.org/html/cs/9906002)
> What is a symbol system? From Newell (1980) Pylyshyn (1984), Fodor (1987) and the classical work of Von Neumann, Turing, Goedel, Church, etc. (see Kleene 1969) on the foundations of computation, we can reconstruct the following definition:

> A symbol system is:

1. a set of arbitrary "physical tokens" scratches on paper, holes on a tape, events in a digital computer, etc. that are
    
2. manipulated on the basis of "explicit rules" that are
    
3. likewise physical tokens and strings of tokens. The rule-governed symbol-token manipulation is based
    
4. purely on the shape of the symbol tokens (not their "meaning"), i.e., it is purely syntactic, and consists of
    
5. "rulefully combining" and recombining symbol tokens. There are
    
6. primitive atomic symbol tokens and
    
7. composite symbol-token strings. The entire system and all its parts -- the atomic tokens, the composite tokens, the syntactic manipulations both actual and possible and the rules -- are all
    
8. "semantically interpretable:" The syntax can be systematically assigned a meaning e.g., as standing for objects, as describing states of affairs).

The author goes on to mention some important consequences of some of these principles.



We begin by considering symbols, and an alphabet. This is foundational for all of logic. We introduce symbols as the first principle

**Principle 1:** There is an *alphabet* which is a set which contain *symbols*. A common example of an alphabet is $\{0,1\}$.

We will not consider any kind of semiotic framework like those of Pierce of Saussure, as the symbols we begin with do not represent or denote anything. They are merely some kind of stipulative symbol which doesn't necessarily have any semantics.

**Definition 1:** An alphabet is a set of symbols

For the purposes of computation we commonly use strings. Strings are a connection of tokenized symbols concatenated with eachother. So for example, the symbols "1" and "0" and "1" concatenates into the string "101".

**Principle 2:** Strings are concatenation of symbols form some alphabet. 

We now define "language". We're not necessarily talking about the English language, or the French language, known as natural languages. We're talking about formal languages. Formal languages are constructed from simple rules that determine the vocabulary of the language. 

**Principle 3:** Rules manipulate strings

With the rule that $A \rightarrow qA$ we can quickly generate a language (using $A$ as a starting string)

$A$
$qA$
$qqA$
$qqqA$
$qqqqA$
...

One can notice that even with simple rules, the language can easily be infinite. Creating finite languages can be done such as.

$A\rightarrow Bb$
$B\rightarrow c$

Then we get
$A$
$Bb$
$cb$

And it terminates. We might eventually introduce states, and develop a string from the empty string as we move from one state to the next. As such, we approach yes, we approach, an end state on which is either accepted or denied.
## Finite state machines 

One way to represent state changes is with a finite state machine. They are generally visualized as many circles with various arrows going between the circles, as a kind of labeled directed graph with 2 special nodes and where the labeling can repeat.

-->a --> (b)

Here, state a is the starting node, and b is the final node. If some input ends at b, it terminates as "yes". Otherwise no. 


They can be used to generate a language, and the finish states determine correctly formed formula.


#### Indeterministic

They can also be non-deterministic, probabilistic and quantum. Each with their own QUIRKS. 

a -->b
   --> c

Same input from a to b and c. How is this resolved? Generally by going over all of the options.

## Turing machines


More complicated rule set

Multiple formalisms

TAPE

The Turing machine is a mathematical construct that has found itself being useful all over the place. Here is a definition
Let $X$ be a list of symbols, whose kleene star operation $X$ denotes a vocabulary, and let $L$ be left, $R$ be right, let $\sigma: \{L, R\}\times X\rightarrow \{L, R\}\times X$ be a function that takes some input and output


## Universal Turing Computing

"Turing also showed that there are universal TMs—machines that can compute any function computable by any other TM. Universal machines do this by executing instructions that encode the behavior of the machine they simulate. Assuming the Church-Turing thesis, universal TMs can compute any function computable by algorithm. This result is significant for computer science: you don’t need to build different computers for different functions; one universal computer will suffice to compute any computable function. Modern digital computers are universal in this sense: digital computers can compute any function computable by algorithm for as long as they have time and memory. (Strictly speaking, a universal machine has an unbounded memory, whereas digital computer memories can be extended but not indefinitely, so they are not unbounded.)"

"The above result should not be confused with the common claim that computers can compute _anything_. This claim is false: another important result of computability theory is that most functions are _not_ computable by TMs (and hence, by digital computers). TMs compute functions defined over denumerable domains, such as strings of letters from a finite alphabet. There are uncountably many such functions. But there are only countably many TMs; you can enumerate TMs by listing each TM specification (which is some finite string). Since an uncountable infinity is much larger than a countable one, it follows that TMs (and hence digital computers) can compute only a tiny portion of all functions (over denumerable domains, such as natural numbers or strings of letters)."
[Computation in Physical Systems (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/ENTRIES/computation-physicalsystems/)


## Example Algorithms (AI related?)





## Chomsky Hierarchy

There are 4 main levels of the Chomsky hierarchy, of which the higher levels contain the lower levels. 





## Undecidability, Halting Problem and the limits of computation

Limits of computation
Turing's undecidability thesis

Similar to Liar's Paradox, Russell's Paradox, Gödel's incompleteness theorem's (note to self: the SEP page does make notes that it's a bit more complicated than just "self-reference"), Tarski's undefinability of truth and the Nelson-Grayling paradox, who all have in common the notion of self-referentiality, there exist a similar problem for computability which is also highly connected and even a lemma of the above one's

Proof:

Imagine a computation C that computes whether or not computations halt. If it feeds itself, it can either say "halts", in which case it halts, but it will never say "halts" if it doesn't halt, it will keep going forever. 


Keep in mind this is only for a general algorithm. Some algorithms do exist that can test the halting of certain algorithms. Just like how Russell's Type theory disallowed Russell's paradox in axiomatic set theory, so can algorithms which specify input types[source]


All of these self-referential paradoxes can be generalized into the fixed point theorem in category theory. 

## Hypercomputation

If hypercomputers existed however, we'd just use brute force algorithms and even do uncomputable algorithms such as AIXI. Of course, how to align an AIXI would still have to be debated, but it would be really easy to have vastly smarter-than-human intelligence.


[Zeno machine - Wikipedia](https://en.wikipedia.org/wiki/Zeno_machine)

Hypercomputation is most likely false as we'll see that it's empirically not the case that we live in as olution to einsteins field equations which allows for closed-timelike curves which could in principle build hypercomputation. But it's controversial to what extent hypercomputation is possible. 




## Computational Complexity: Can a big enough computer with enough time just solve any decidable problem?



P = NP discussion

ExpSpace

Quantum Complexty and quantum supremacy?

Brute force

AI as a P-algorithm heuristic that's a "good enough" approximation of solutions to NP-complete problems.

[Computability and Complexity (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/computability/)
"With a very few exceptions, when a natural problem is in NP, it turns out to be either NP complete, or in P. People wondered whether this was always the case. Ladner proved that if P ≠≠ NP, then there is an intermediate problem I� that is in NP but not in P and not NP complete [Ladner, 1975]."

"Ladner’s proof method is called delayed diagonalization. He generalized his result to show that for any two well-behaved complexity classes, C, E such that C is strictly contained in E, there is an intermediate class, D such that C is strictly contained in D and D is strictly contained in E. Thus the ordering of complexity classes is dense."

"However, these intermediate problems don’t tend to show up."

Idea: Similar to the Continuum hypothesis, given that we don't see sets between $\aleph_0$ and $\aleph_1$. 



[philos.pdf (scottaaronson.com)](https://www.scottaaronson.com/papers/philos.pdf)
![[Pasted image 20240808213558.png]]


![[Pasted image 20240809141903.png]]

![[Pasted image 20240809142446.png]]

Look up tables for Searle's chinese room thought experiment. Grows exponentially with conversation length $n$. Humans may have evolved/developed efficient algorithms for language.

Waterfal/Putnam all possible computations thingy



# Logic - Computation correspondences
Do I need this section?

Logic, such as first order logic- has an equivalent computation that proves all theorems in that system. Proof systems are in that sense computational. 

## Curry Howard (Lambek) Correspondence
As such, the lambda calculus has a correspondence with first order intuitionistic logic.



# Metalogic

## Compactness

## Completeness
## Incompleteness Theorems

This won't be brought up until the chapter in [[Mind (Other)]], so it's safe to skip this chapter for now if you wanna read about physical realizability of computation. 

Gödel's incompleteness theorem's are 2 very important theorems that proove certain metalogical conditions that logical systems are must abide by. They state that all logical systems, who are $\omega-$consistent with sufficient arithmetic (we'll go back to what exactly this means), have true sentences that are unprovable withing the system. The second incompleteness theorem states that such systems cannot prove that they are themselves consistent. 

It's worth noting that this does *not* entail *all* of mathematics is incomplete, only certain logical systems. There's a list [on wikipedia] on all provably complete and consistent systems, such as finitist set theory, the theory of random graphs and Robinson arithmetic. 


 Gödel wanted to generalize the incompleteness theorem from principica mathematica. He apparently could use Turing machines for this as a way to model the very general system of effective procedure, allowing for the fully general theorem which states that consistent systems with sufficient arithmetic (and finitely axiomatizable?) can have true but unprovable propositions. 
Source: Section 3.1 SEP page churc-turing thesis



## Brief, informal overview
Yes (probably more logic than math but whatever, we'll see how I organize the chapters)

Needs to be atleast arithmetic $Q$ for gödel numbering (given that it requires the prime factorization theorem)

Needs to be $\omega-$consistent

Needs to be recursively axiomatizable (i.e there exists a program $P$ that decides $P(ax)$ is true iff $ax$ is an axiom)

Diagonalization lemma states that for all gödel-numbered predicates $A$, it is the case that a propsosition $G$ can be created such that 
$F\vdash G\leftrightarrow A([G])$

Apply it to the "not provable" predicate

$F\vdash G\leftrightarrow \neg PROV([G])$

BOOM!
Assume it's not provable yada yada
Assume it is yada yada





Rosser's Theorem via Turing machines, connections with Gödel incompleteness theorem.
https://scottaaronson.blog/?p=710
[Shtetl-Optimized » Blog Archive » Rosser’s Theorem via Turing machines (scottaaronson.blog)](https://scottaaronson.blog/?p=710)


"For those of you who’ve never seen the connection between these two triumphs of human thought: suppose we had a sound and complete (and recursively-axiomatizable, yadda yadda yadda) formal system F, which was powerful enough to reason about Turing machines.  Then I claim that, using such an F, we could easily solve the halting problem.  For suppose we’re given a Turing machine M, and we want to know whether it halts on a blank tape.  Then we’d simply have to enumerate all possible proofs in F, until we found _either_ a proof that M halts, _or_ a proof that M runs forever.  Because F is complete, we’d eventually find one or the other, and because F is sound, the proof’s conclusion would be _true_.  So we’d decide whether M halts.  But since we already know (thanks to Mr. T) that the halting problem is undecidable, we conclude that F can’t have existed."

“Look ma, no Gödel numbers!”