

- Narrow: Word2Vec, Mini-world agents
- Cyc, databases, formal ontology
- General: Gödel machines, AIXI



# Narrow AI


# Automated reasoning

## The first AI models: Automated Deductive Reasoning

Theorem provers may no longer be seen as AI, but at the time they were considered as such. This is likely because they were performing deductive reasoning, one of three important inference methods (among inductive and abductive, though there's a debate as to which extend abductive reasoning is reducible to induction or both of the other two [soprcef)]. 

So,, multiple automated theorem proovers would be built over time. Starting with this and that yadayada

They work based on proof theory. You take some inference system with inference rules, you add some axioms, and start deriving something, such as a proof for something you want, or just let it explore the space of derivations.

You get these results as an example:
[Actual example's I will do myself]

Theorem provers are still used among mathematicians, logicians and philosophers, but are not in common usage anymore. 

Note: Will I write about proof theory in say, [Logic]?

## Automated Inductive Reasoning

Of course, deductive reasoning on its own is not enough. All it does is allow you to infer conclusions from premises, given some inference rules. It doesn't tell you what really is true, which axioms are correct. Perhaps one way of doing it is via inductive reasoning?


For example if we have
$Mat(x_1)\wedge Cat(x_1)$
$Mat(x_2)\wedge Cat(x_2)$
$Mat(x_3)\wedge Cat(x_3)$

Then we derive inductively that $\forall x Mat(x)\rightarrow Cat(x)$

Of course, this shows the problem quite clearly as Hume pointed out earlier[source] that constant conjunction for a finite number of observations is not enough to demonstrate a universal generalization. It is possible, since it is conceivable or atleast consistent, that there exists some object $x_k$ such that $x_k$ is on a mat yet not a cat. 

Only utilizing these generalizations is not enough. 



#### Formal Learning Theory

Formal Learning Theory (FLT) is one way to characterize hypothesis testing, with a focus on the new riddle of induction, in a formal mathematical way. 

This way it can handle the new problem of induction formally.


## Automated Abductive Reasoning

So how it works is that out of sense-data propositions one can form propositions about observations. So we have observation sentences $O_1,\dots O_2$. They may be of the form $Fx_1\wedge Gx_1$ such that one can derive $\forall x Fx\rightarrow Gx$. The idea is to form theoretical propositions which derive these laws and via Ramsey sentences connect the theoretical sentences with the observational. $$T_1\wedge\dots\wedge T_n \wedge R_1\wedge\dots R_k \vdash O_1\wedge\dots O_m$$


Yet such an AI is today is obviously woefully unprepared for the uncertainty in the world, and so on. That's why their project failed too. Now to a solution.

Well go back to naive solutions when we go into Popperian AI aswell. Let an AI model exist such that it has a set of propositions $T$ and a set of evidence $E$. 

Although just because we have an algorithm doesn't mean it's easy. Time complexity can still be ridiculously high. 

So perhaps it is the case that since the logical positivists never managed to formalize learning and understanding, that as a consequence they weren't able to engineer an epistemological machine. 


[Hempel article on machine hypothesis finding]






# Wide AI, AGI


# General Seed AI systems

## AIXI
![[Pasted image 20240204125552.png]]

Insert the slide from my phil of science professor with the lecture on events and laws which compress the information.

[Artificial Intelligence > The AIXI Architecture (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/artificial-intelligence/aixi.html)

uncomputable bro. If weh ad access to hypercomputation, it seems like we could run an AIXI until it figures out an optimal solution for maximising utility. So while we have argued that [hypercomputation is likely not nomonologically possible], if anyone ever finds closed timelike curves, stay on the lookout for anyone trying to implement an AIXI model into it. 


Apparently, the orthogonality thesis has been shown true for AIXI!? 

## Gödel Machine

Somewhat self-referential kekw.

## Modal Fusion Systems
Modal Fusion systems have to high time-complexity

So for instance in modal logic one can simply state that $Kp$ to mean that "it is known that $p$". This can be amended in multi-agent epistemic logic with things like $K_1p\wedge \neg K_2p$ in which case agent 1 knows p but agent 2 doesn't know p. 

With fusion logics, one can state the typical JBT definition of knowledge by fusing it with doxastic and justification logic. [Fusion systems]

$$Kp \leftrightarrow (Bp\wedge Jp \wedge p)$$
If we merely a priori reason about what justifies belief, we could simply build an AI that way? This is not entirely possible, because what makes something justificatory is more of an empirical question than something that can be reasoned from armchair philosophy. 






![[Pasted image 20241205143055.png]]

Gödel machines


Move these to the logic chapter?
Computational coherentism
Kolmogorov Induction
Singular Learning Theory

Singular Learning theory
[[Singular Learning Theory]]


### Aristotelean AI/NARS

One way to formalize epistemology is to formalize the different kinds of syllogisms: deductive, inductive, abductive and self-justification. This is the exact model one AI system uses called [fasfaafs]. This is cool






## Representation AI

Let's say you want to represent the external world. Representations stores properties about objects, such that for all object $x$ with $\{P_1x,\dots P_nx\}$ aswell as relational properties with other objects $\{R_1(x_1,\dots x_n),\dots R_m(x_{k},\dots x_w)\}$. 

Example of such ontology may be "The cat is on the mat" or $OnMat(Cat, Mat)$ and that the AI is merely preloaded with this kind of "common sense" knowledge of the world. 


# Frame Problem
But this leads to the frame problem. Given any problem, there's likely many objects involved, and as such is very slow in being able to bring up the relevant properties and relations. 



We can fix this!

Hilary Putnam, John McCarthy

# The symbol grounding problem




[The Symbol Grounding Problem](https://arxiv.org/html/cs/9906002)



