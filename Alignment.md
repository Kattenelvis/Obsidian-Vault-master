



https://futureoflife.org/open-letter/pause-giant-ai-experiments/ 
#### Notes and references

[1]

Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ü¶ú. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).

Bostrom, N. (2016). Superintelligence. Oxford University Press.

Bucknall, B. S., & Dori-Hacohen, S. (2022, July).¬†[Current and near-term AI as a potential existential risk factor.](https://arxiv.org/abs/2209.10604)¬†In¬†_Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_¬†(pp. 119-129).

Carlsmith, J. (2022).¬†[Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353). arXiv preprint arXiv:2206.13353.

Christian, B. (2020). The Alignment Problem: Machine Learning and human values. Norton & Company.

Cohen, M. et al. (2022).¬†[Advanced Artificial Agents Intervene in the Provision of Reward.](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)¬†_AI Magazine_,¬†_43_(3) (pp. 282-293).

Eloundou, T., et al. (2023).¬†[GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.](https://arxiv.org/abs/2303.10130)

Hendrycks, D., & Mazeika, M. (2022).¬†[X-risk Analysis for AI Research](https://arxiv.org/abs/2206.05862). arXiv preprint arXiv:2206.05862.

Ngo, R. (2022). The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626.

Russell, S. (2019). Human Compatible: Artificial Intelligence and the Problem of Control. Viking.

Tegmark, M. (2017). Life 3.0: Being Human in the Age of Artificial Intelligence. Knopf.

Weidinger, L. et al (2021). Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.

[2]

Ordonez, V. et al. (2023, March 16).¬†[OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks: 'A little bit scared of this'](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122). ABC News.

Perrigo, B. (2023, January 12).¬†[DeepMind CEO Demis Hassabis Urges Caution on AI](https://time.com/6246119/demis-hassabis-deepmind-interview/). Time.

[3]

Bubeck, S. et al. (2023).¬†[Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712). arXiv:2303.12712.

OpenAI (2023).¬†[GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf). arXiv:2303.08774.

[4]

Ample legal precedent exists ‚Äì for example, the widely adopted¬†[OECD AI Principles](https://oecd.ai/en/dashboards/ai-principles/P8)¬†require that AI systems "function appropriately and do not pose unreasonable safety risk".

[5]

Examples include human cloning, human germline modification, gain-of-function research, and eugenics.


https://www.reddit.com/r/singularity/comments/ybbpa1/ai_alignment_through_properties_of_systems_and/

Bing wants to destroy humanity
[https://www.youtube.com/watch?v=Pk0iI0Lrg9Y](https://www.youtube.com/watch?v=Pk0iI0Lrg9Y "https://www.youtube.com/watch?v=Pk0iI0Lrg9Y")


"Alignment and truthfulness seem synergistic. If we knew how to build aligned systems, this could help building truthful systems (e.g. by aligning a system with a truthful principal). Vice-versa if we knew how to build powerful truthful systems, this might help building aligned systems (e.g. by leveraging a truthful oracle to discover aligned actions)." From Truthful AI

"t might even be that since truthfulness is a clearer and narrower objective than alignment, it would serve as a useful instrumental goal for alignment research."

"A key challenge for implementing truthfulness rules is that nobody has full knowledge of what‚Äôs true; every mechanism we can specify would make errors. A worrying possibility is that enshrining some particular mechanism as an arbiter of truth would forestall our ability to have open-minded, varied, self-correcting approaches to discovering what‚Äôs true. This might happen as a result of political capture of the arbitration mechanisms ‚Äî for propaganda or censorship ‚Äî or as an accidental ossification of the notion of truth."




### The value alignment problem: a¬†geometric approach
"
**The weak value alignment** **thesis**: Autonomous systems should be designed in ways that are beneficial for humans. When human values (or interests or preferences) clash with other values, autonomous sys tems should give preference to human values (or interests or preferences). 

**The strong value alignment thesis**: Autonomous systems should be designed in ways that are beneficial for humans, as specified in the weak value alignment thesis, and at each point in time t the best way to do this is to align the values of autonomous systems with the values (or interests or preferences) that humans actually embrace at t.

The weak value alignment thesis is anthropocentric. It emphasizes the values, interests and preferences of human beings. Anthropocentric moral theories attract considerable controversy. Environmental ethicists distinguish between anthropocentric views and biocentric ones that stress the values or interests of nonhuman organisms, such as animals, plants or ecosystems. Advocates of biocentric views believe we should align autonomous systems with biocentric values rather than the anthropocentric ones currently embraced by many members of the human race."

"**the epistemic value alignment thesis**, according to which we should accept the second but not the first part of the strong thesis. On this view, the values of autonomous systems should be aligned with our values at time t no matter what those values are. The idea behind the epistemic theses is that we should use whatever values we actually embrace as templates for autonomous systems. What we currently value is what we have most reason to believe autonomous sys tems ought to value. According to this epistemic value align ment thesis we should, thus, align the values of autonomous systems with our own values even if those values turn out to be biocentric."


"I believe these two phenomena, the prevalence of evalua tive uncertainty and human irrationality, show that we ought to reject the strong value alignment thesis"

"A reasonable ver sion of the value alignment thesis has to account for the fact that human beings do not always know what is right or wrong and do not always behave rationally. This brings me to the following moderate formulation: 

**The moderate value alignment thesis** Autonomous systems should be designed in ways that are beneficial for humans, as specified in the weak value alignment thesis, and at each point in time t the best way to do this is to align the values of autonomous systems with some of the values (or interests or preferences) that humans actually embrace at t."


"That said, it of course remains to explain what human values advocates of the moderate thesis should select as tem plates in a rational value alignment process. My preferred solution, discussed in ‚ÄúConceptual spaces and paradigm cases‚Äù, is to single out a small number of moral paradigm cases for training autonomous systems. In essence, I propose that we should instruct autonomous systems to base evalu ations of nonparadigmatic cases on how similar they are to paradigm cases. If the autonomous system chooses between two options, it should reason in the same way as in the most similar paradigm case. These similarity relations can be rep resented in a multidimensional geometric space"



"We do not know on a societal level, which ethical theory we have most reason to accept. There is no consensus among moral philosophers on which ethical theory is correct. Moreover, the idea that each group in society should be free to design its own utility function would entail a rather extreme form of moral relativism, which hardly anyone is willing to accept. It would make little sense to say: ‚ÄúMy autonomous car is utilitarian and therefore protects pedestrians on the sidewalk as much as its occupants, but it is perfectly okay that your car is Aristotelian and gives priority to your friends in the backseat.‚Äù"


"The problem is that we, the collective of agents design ing autonomous systems, do not know which ethical theory we have most reason to accept. Needless to say, this skeptic insight does not entail that all theories are false, or that no single individual knows which theory is correct. The fact that you and I subscribe to different (non-equivalent) ethical theories entails that we as a group do not know which theory is correct. Moreover, it seems unlikely that we could solve this problem within the foreseeable future. The most likely outcome of further research efforts would be that we end up with an even larger number of theories to choose from, but hardly any decisive reasons for eliminating any of the theories that are currently on the list. In light of all this, I propose that the best way forward, at least for the moment, **is to design autonomous systems without taking any stance on which ethical theory we have most reason to accept**."


They wrote a book on geometrical ethics

The suggestion that moral conclusions should be based on comparisons with paradigm cases is not new. Aristo tle famously pointed out that we should ‚Äútreat like cases alike‚Äù24, and for hundreds of years casuists (many of whom were affiliated with the Catholic Church) used this idea for arguing that moral conclusions should be based on how similar a new moral choice situation is to some previously analyzed paradigm cases.25 The novel element of the present discussion is the suggestion that (i) paradigm cases can be used for calibrating the values of autonomous systems, and that (ii) moral principles can be modelled in autonomous systems as Voronoi tessellations defined by paradigm cases. To illustrate, we can observe briefly how the moral princi ples articulated in ET have been construed and tested empiri cally. In the book, I propose that engineers who design and use new and existing technologies ought to be guided by the following five principles: 

1. The Cost-Benefit Principle (CBA)26 
2. The Precautionary Principle (PP)27 
3. The Sustainability Principle (ST)28 
4. The Autonomy Principle (AUT)29 
5. The Fairness Principle (FP)30

