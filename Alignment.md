



https://futureoflife.org/open-letter/pause-giant-ai-experiments/ 
#### Notes and references

[1]

Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ðŸ¦œ. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency (pp. 610-623).

Bostrom, N. (2016). Superintelligence. Oxford University Press.

Bucknall, B. S., & Dori-Hacohen, S. (2022, July).Â [Current and near-term AI as a potential existential risk factor.](https://arxiv.org/abs/2209.10604)Â InÂ _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_Â (pp. 119-129).

Carlsmith, J. (2022).Â [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353). arXiv preprint arXiv:2206.13353.

Christian, B. (2020). The Alignment Problem: Machine Learning and human values. Norton & Company.

Cohen, M. et al. (2022).Â [Advanced Artificial Agents Intervene in the Provision of Reward.](https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/15084)Â _AI Magazine_,Â _43_(3) (pp. 282-293).

Eloundou, T., et al. (2023).Â [GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.](https://arxiv.org/abs/2303.10130)

Hendrycks, D., & Mazeika, M. (2022).Â [X-risk Analysis for AI Research](https://arxiv.org/abs/2206.05862). arXiv preprint arXiv:2206.05862.

Ngo, R. (2022). The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626.

Russell, S. (2019). Human Compatible: Artificial Intelligence and the Problem of Control. Viking.

Tegmark, M. (2017). Life 3.0: Being Human in the Age of Artificial Intelligence. Knopf.

Weidinger, L. et al (2021). Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.

[2]

Ordonez, V. et al. (2023, March 16).Â [OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks: 'A little bit scared of this'](https://abcnews.go.com/Technology/openai-ceo-sam-altman-ai-reshape-society-acknowledges/story?id=97897122). ABC News.

Perrigo, B. (2023, January 12).Â [DeepMind CEO Demis Hassabis Urges Caution on AI](https://time.com/6246119/demis-hassabis-deepmind-interview/). Time.

[3]

Bubeck, S. et al. (2023).Â [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712). arXiv:2303.12712.

OpenAI (2023).Â [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf). arXiv:2303.08774.

[4]

Ample legal precedent exists â€“ for example, the widely adoptedÂ [OECD AI Principles](https://oecd.ai/en/dashboards/ai-principles/P8)Â require that AI systems "function appropriately and do not pose unreasonable safety risk".

[5]

Examples include human cloning, human germline modification, gain-of-function research, and eugenics.


https://www.reddit.com/r/singularity/comments/ybbpa1/ai_alignment_through_properties_of_systems_and/

Bing wants to destroy humanity
[https://www.youtube.com/watch?v=Pk0iI0Lrg9Y](https://www.youtube.com/watch?v=Pk0iI0Lrg9Y "https://www.youtube.com/watch?v=Pk0iI0Lrg9Y")


"Alignment and truthfulness seem synergistic. If we knew how to build aligned systems, this could help building truthful systems (e.g. by aligning a system with a truthful principal). Vice-versa if we knew how to build powerful truthful systems, this might help building aligned systems (e.g. by leveraging a truthful oracle to discover aligned actions)." From Truthful AI

"t might even be that since truthfulness is a clearer and narrower objective than alignment, it would serve as a useful instrumental goal for alignment research."

"A key challenge for implementing truthfulness rules is that nobody has full knowledge of whatâ€™s true; every mechanism we can specify would make errors. A worrying possibility is that enshrining some particular mechanism as an arbiter of truth would forestall our ability to have open-minded, varied, self-correcting approaches to discovering whatâ€™s true. This might happen as a result of political capture of the arbitration mechanisms â€” for propaganda or censorship â€” or as an accidental ossification of the notion of truth."
