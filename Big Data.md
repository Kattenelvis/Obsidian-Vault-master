[Scientific Research and Big Data (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/science-big-data/)

The end of theory, just induction?

Suppsed formal definition:

> _Z_ is an N-fold model of the data for experiment _Y_ if and only if there is a set _Y_ and a probability measure _P_ on subsets of _Y_ such that Y=⟨Y,P⟩Y=⟨Y,P⟩ is a model of the theory of the experiment, _Z_ is an N-tuple of elements of _Y_, and _Z_ satisfies the statistical tests of homogeneity, stationarity and order. (1962: 259)


> In a forceful critique informed by the philosophy of mathematics, Christian Calude and Giuseppe Longo argued that there is a fundamental problem with the assumption that more data will necessarily yield more information:

> very large databases have to contain arbitrary correlations. These correlations appear only due to the size, not the nature, of data. (Calude & Longo 2017: 595)

> They conclude that big data analysis is by definition unable to distinguish spurious from meaningful correlations and is therefore a threat to scientific research. A related worry, sometimes dubbed “the curse of dimensionality” by data scientists, concerns the extent to which the analysis of a given dataset can be scaled up in complexity and in the number of variables being considered. It is well known that the more dimensions one considers in classifying samples, for example, the larger the dataset on which such dimensions can be accurately generalised. This demonstrates the continuing, tight dependence between the volume and quality of data on the one hand, and the type and breadth of research questions for which data need to serve as evidence on the other hand.



[Scientific Research and Big Data (Stanford Encyclopedia of Philosophy)](https://plato.stanford.edu/entries/science-big-data/)
