An iterated prisoners dilemma is a set of $n$ prisoner dilemmas between two agents. Agents can at each individual dilemma choose to defect or cooperate, and as standardly is the case they either both gain by cooperating, both loose by defecting, and if one defects and the other cooperates, the defecting one wins the most. 

A way of calculating rational choices in iterated prisoners dilemmas (and sequential games in general), namely backwards induction, suggests that agents should defect in every round. The argument goes like this: Assume that agents 1 and 2 are rational, both believe that they are rational, both believe that the other is rational and so on (i.e rationality is a common belief). In that case, 1 will reason that they should defect in the last round, i.e round $n$, since any cooperative action taken at $n$ won't matter. By the common belief of rationality, agent 1 would reason that 2 would reason that agent 1 would defect at round $n$, and then do the same in round $n-1$, hence 1 ought to defect there too. This argument continues all the way back to round $1$, hence they ought to always defect.

The issue some point out is an intuition that a tit-for-tat strategy, namely one in which you begin with cooperation and defect if your opponent defects, is the actually rational choice. Pettit and Sudgen argues that in fact, cooperating in the first round is a rational choice. The core of their argument is that the common belief assumption breaks down in the case of cooperation in the first round. That is to say, the act of cooperation induces a change in beliefs in a way that violates the common belief in their rationality. If I were to cooperate, then my opponent would no longer believe that I'm rational, or believe that I believe my opponent is rational and so on. A belief shift would make the common belief assumption mistaken. 

The backwards induction argument can thus no longer be a sound argument. When we remove the assumption that the common belief in rationality will remain throughout the game, no matter what the agents do, and instead take common belief as a statement that could get revised, then the backwards induction argument cannot hold. There will be situations in which agents would cooperate, for instance by signaling their willingness to cooperate, which breaks down that assumption. It would only hold if the players take actions that uphold the common belief in rationality.

It's worth noting that the authors are not arguing that cooperating in the first round is the uniquely rational choice, just a rational choice. If cooperating in the first round was uniquely rational, then given that both players are rational and that they have a common belief in their rationality, they would both cooperate. This would have the same effect as the belief change, hence common belief in rationality would not survive. 

My own evaluation is the following. I've always had a bit of a suspicion that common belief and common knowledge are unrealistic assumptions to have on finitely bounded agents. The idea that agents hold an infinite number of beliefs, or a belief that's infinitely long, or however you wanna put it, is unrealistic given that it is unrealistic that agents are not finitely bounded. Of course, there are ways around this, as David Lewis argues that common belief is attainable from a finite set of beliefs and axioms which generate the common belief, for instance via public events. However I personally commit to ultrafinitism about logic, so even this won't do well given that commitment, however it's obviously a lot more of a controversial position to hold and most won't see it as a genuine argument against common knowledge.

I also agree with the conclusion that a tit-for-tat strategy is a rational choice. Descriptive game theory may be a different field from the theoretical aspects of game theory, yet it would be interesting to see citations on empirical studies on success rates of different strategies, via simulations. Alternatively, seeing real life reasoning about these aspects, especially among more well read people with time on their hands to optimize for some strategy. It would be interesting to see their thinking process. Outsmarting their opponents in clever ways, and the, as they say in the article, vast underdetermination in what they choose and their thinking strategy behind their choice. The common belief in rationality would already be avoided within a field study of this sort. The null hypothesis I hold is that tit-for-tat behaviour would in the long run be the most effective, including one's where the agents form trust and cooperate on the last time-step. This kind of trust formation should probably be added as part of the formalism aswell. 





Number 2 text

Backward induction centipie game

coins from a pile

first player take two coins, preventing progress

1. common belief in rationality
2. no false beliefs
3. retention of true beliefs

rationality can be maintained

