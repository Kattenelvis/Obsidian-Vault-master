

AGI is possible: No other feasible argument than lucas-penrose. 

Chinese Room: Limited to some models of the time, symbol grounding problem is solvable

Dreyfus: Limited to some models of the time, frame problem is solvable (possibly even without embodiment)



The singularity argument (Chalmers)

A --> A+ -->A++ ....



Bostr√∂m-Omohundro theory of rationality. 

1. Orthogonality Thesis: Strong version: Any goal is compatible with any level of intelligence. Probably false, a weaker version excludes trivialities 

2. Convergent instrumentality: A small set of subgoals will be useful for a very large set of end goals.





In short:

AGI is possible (feasible, likely)
If AGI is possible (feasible, likely), then ASI is possible (feasible, likely)
if ASI is possible (feasible, likely), then human extinction risk from ASI is possible (feasible, likely)


