

![[Representation.excalidraw]]



Representational theory of mind states that all mental content is a representation
Functionalism states that all representations are computational (ran on a Turing Machine)
Identity theory states that all mental content is identical with a way a material substrate is structured.

A mental representation is a state of belief, desire (e.t.c?) of a certain proposition p. The objects of the sentence is the content of a representation, but there's debates regarding the existence of contents of representations. An example is "dog", and it's potential relation to mental images of a dog (what about aphantasia?). 

Thought is the processing of mental representations.

When a human walks, mental representations play a role in how it's walking. This occurs without awareness/consciousness. Sometimes missrepresentations can cause problems, like having a missrepresentation about a floor. Misrepresentation explains why behaviour fails to go smoothly or fails to meet an agent’s needs or goals. When things go badly for an agent, we can often pin the blame on an incorrect representation.

Disjunction problem 
$dog\vee \{fluffy,barker\}$

We can never get a wrongrepresentation if we just conjunctg every set of properties. 

Furthermore, a disjunction of conditions is always more likely than conditions taken individually: for example, an object might be an eagle, but it is more likely that it is an eagle or a crow. So, content-as-probability-raising faces a particularly acute form of the disjunction problem.

Another tactic looks to relations between representations to fix content. We saw the idea earlier that the concept dog gets its meaning from the inferences by which it is formed, such as from perceiving a brown furry object, and perhaps also from conclusions it generates, such as inferring that this thing might bite me (Block 1986). Patterns of inferences are plausibly what changes when a child acquires a new mathematical concept (Carey 2009). They are also the focus of recent Bayesian models of causal learning (Gopnik and Wellman 2012, Danks 2014) Moving beyond beliefs to neural representations, dispositions to make inferences—that is, to transition between representations—could fix content here too. If all inferences are relevant, holism threatens (Fodor and Lepore 1992): any change anywhere in the thinker’s total representational scheme would change the content of all of their representations. There have been attempts to identify, for various concepts, a privileged set of dispositions that are constitutive of content (e.g. Peacocke 1992).

Correspondence theory: Structural correspondence between the representation of two objects on a map and it's spatial relation to eachother can correspond to the representation of two objects in the world and their spatial relation (Functors?!)

Ascriptionism theory: 


Informational approaches to content direct our attention to the way a representation is produced. Conditions in the world cause a representation to be tokened;1

There exists cognitive modules that are consumers of mental reprsentations. A consumer inputs representations and outputs behabiour (maybe representations aswell?)

To see what a person believes, see how they act to satisfy their desires


Distinction: Representations of and representations as

Further, the proponents of teleological theories rarely believe that referential content is a kind of narrow content, and so they are not usually offering theories of narrow content. How to characterize narrow content is controversial, but the proponents of teleological theories tend to agree with those who think that two beings who are physical replicas at a time _t_ “from the skin in”, so to speak, can differ in the referential contents of their mental states at _t_. Of course, this view is also shared by plenty of other philosophers, who think that mental reference to content supervenes (in part) on things that are external to the individuals whose mental states are in question, such as on features of their social or physical environments and/or learning histories (perhaps for the reasons given by Putnam 1975 and Burge 1979, 1986)


"to do that I will set aside some features of everyday representations that make the content question more complex. Consciousness is one. I won’t deal with cases where a representation’s being conscious is relevant to fixing its content. A second feature of ordinary occurrent beliefs and desires1 is that they enter into epistemic relations: perceptual states justifying beliefs, beliefs justifying other beliefs, and so on. I set aside cases where a representation’s entering into relations of justification for the person are relevant to its content. A third feature, which is probably related, is that they are offered as reasons in the social process of explaining to others what we believe and why we act as we do. When acting together, these reasons or verbal reports feature heavily in deliberative joint decision-making. A fourth feature is the kind of constituent structure characteristic of natural language; for example, if there were a kind of representation only available to those capable of using genuinely singular terms."

neural mechanisms for accumulating probabilistic information about reward (Yang and Shadlen 2007) and the neural circuit responsible for motor control (Wolpert et al. 1998, Franklin and Wolpert 2011). Figure 2.1 illustrates the latter case.

Since non-conscious, non-doxastic neural representations raise the problem of content in a clear way, one central aim of the book is to provide a theory of content for them. 

Representational Theory of Mind:
mental representations are physical particulars which interact causally in virtue of non-semantic properties (e.g. their physical form) in ways that are faithful to their semantic properties.7 Psychological processes like thinking, perceiving, planning, reasoning and imagining consist of causal processes taking place between representations with appropriate contents.

According to RTM, transitions between mental representations are faithful to their contents, amounting to an algorithm by which a system achieves its functions. It is not just by chance, or through a look-up table, that the system instantiates a useful input– output mapping. It implements an algorithm and, if more than one algorithm would produce the same mapping, there is a fact of the matter about which one the system is in fact using.

"Task functions need not be generated by representations of, for example, conditions, goals, or targets."
This coulx imply that representations are not just desires and beliefs, but also conditoins, goals and targets. Perhaps other things too like ethical statements (shouldn't do $p$ for example).

Mental representations stand in exploitable relations with an external world, 

tabilized Function An output F from a system S is a stabilized function of S iff producing F has been systematically stabilized: (i) by contributing directly to the evolutionary success of systems S producing F; or (ii) by contributing through learning12 to S’s diposition to produce F; or (iii) where S is an organism, by contributing directly to the persistence of S.


Task Function An output F from a system S is a task function of S iff (a) F is a robust outcome function of S; and (b) (i) F is a stabilized function of S; or (ii) S has been intentionally designed to produce F.


A [mental representation](https://plato.stanford.edu/entries/mental-representation/) is a mental item with _semantic properties_ (such as a denotation, or a meaning, or a truth-condition, etc.).



![[Pasted image 20230720105358.png]]
I wonder how this relates to the Chinese room experiment


![[Pasted image 20230720113228.png]]

![[Pasted image 20230720113250.png]]

![[Pasted image 20230720113310.png]]
TODO: Figure out why cognitive phenomenology imply all these things. 


Non-Conceptual representations divide into representationalist views and phenomenalist views. 
![[Pasted image 20230720121813.png]]


![[Pasted image 20230720122143.png]]

It seems as if representationalists are more closely aligned to direct/naive realism and external world realists while phenomenalists are more closely aligned with sense-datum theory and external world scepticism.

![[Pasted image 20230720123552.png]]
Ganzfield experiences


![[Pasted image 20230720131424.png]]
Ummm very common Chalmers W?


Computational theory of mind:
![[Pasted image 20230720140355.png]]
![[Pasted image 20230720140726.png]]


[(87) Computation and Representation - YouTube](https://www.youtube.com/watch?v=DgvV3IbUedo)
Mental Imagery Representation
Linguistic Representation
Symbolic Representation

![[Pasted image 20230720135013.png]]
