
# Basic Values and Rationality
I value certain things intrinsically. I introspect and model my preference relation. It is rational to climb the preference relation, so I take actions that maximise my probability of doing so.

# The case for Transhumanism
It is generally more effective to do this connected to a virtual world (hereafter "vorld"). Everyone can accept some kind of principle that is "I get my own Vorld, you get your own Vorld". Liberal Transhumanism (but don't call it that?). We need to prevent unnecessary copying of minds, to prevent malthusian resourcelack and some repugnant conclusion like situation.

Chalmers and virtual objects. He held up a glass of water and said "if we're in a simulation, this glass of water is not fake, it is just virtual. We've merely learnt a new property about the object". A virtual world is not fake, it is real, and it's certiantly a real experience. A full mind upload into such a matrix would be good, if we're in control of a good experience, and needless involuntary suffering is not a part of it. 

We empower such virtual worlds with dyson spheres mined through the dismantlement of planets. It is worth noting that environmentalism, the protection of the environment, as an instrumental good to protect humanity, not an intrinsic good. I do not value nature any more than a vorld that looks like nature, it is the same. We can save astronomical environments, including (and in my opinion, especially) earth. But we should have no qualms about dismantling earth. It is the final solution to wild animal suffering while we're at it. 

We live for longer, becuase a good life for a longer period of time is even better. Short life is not what makes it good, we can keep a goodness level of $w$ whether or not we die at $T$ or $T* >> T$

# Accelerate Technology?
The goal: Accelerate Technology

The rationality case: I get to climb my preference relation
The ethics case: Nick Bostr√∂m's Astronomical Waste article. Avoid suffering. Same with deontological views (allows less lies etc.) and virtue ethics case (we can grow more with technology). Contractarianism? Anyways, moral uncertianty, some probability, yeah. 

# Minimizing X-Risk
Some technologies can be dangerous (atom bomb etc.) even threatening all human life (X-risk). Aswell as S-risk. S-risk is likely very rare, uncalculable probability. 

Artificial general intelligence, and artificial superintelligence, can both accelerate technological progress and can be dangerous (X-risk and S-risk).

There's both a rationality case and an ethics case for minimizing risk, and that minimizing risk is more important than technological acceleration.

The risk is 2-10% according to papers. We should minimize it. You wouldn't step on a plane with 10% probability of going down!

# Policy and AI governance
EU policy: Fund AI safety, co-operate on global AI pause (realism vs liberalism in IR might impact how feasible this is)

Sending more people to the EU parliament who is concerned about AI safety. Focus on them. Vote on them. Become one of them. 

How to become one of them? Play the political game, learn politics, study political science, join parties. Form coalitions (logic of coalitions?). Tit-For-Tat?


This sucks! I don't really like this! I'm bad with people, I cannot keep track of them, bad at keeping up conversations, bad at lying. I just wanted to study the philosophy of mind and formal logic. Well-well...

