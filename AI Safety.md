

"AI safety is only popular among companies to increase profits"

After studying Open AI's governance course, this seems false. 

Short term risks involve human misuse of AI that generates harm or limits our potential in some way, for instance developing new bioweapons, nuclear missiles, misinformation campaigns e.t.c. Long term risks include misalignment, loosing control, and mediocre values lock in. 

While I'm normally a socialist, when it comes to companies one could analyze it using methodological individualism from analytic Marxism. The people in companies have incentives to not die, or otherwise be harmed by the aforementioned risks. As such, they are incentivized to cooperate with other institutions, including competing companies, in order to slow down AI development at focus on AI safety. 

The spread of AI safety ideas began in some corners on the internet, such as the rationality community and some transhumanist communities. This eventually spread to effective altruism, academia, and the bay area, and over the last couple years or so the memetic replication rate has become very large. By the time it had gotten popular in the bay area, which is fairly recent, it would also start to become popular among people in leading corporate roles. They are likely genuinely scared, or atleast make the calculated decision to limit AI capabilities that could cause harm to themselves. 

This is different from say, fossil fuel corporations with global warming, since it has a vastly longer time-scale and higher levels of certainty regarding which kinds of damages will occur.  




Work opportunities 
[Lightcone Infrastructure](https://www.lightconeinfrastructure.com/)
[Come work with us! (conjecture.dev)](https://www.conjecture.dev/career/)


University collaborations [Berkeley Existential Risk Initiative (existence.org)](https://existence.org/)



[AI Safety Training](https://aisafety.training/)

[SERI ML Alignment Theory Scholars Program (serimats.org)](https://www.serimats.org/)

[Research Projects | CAIS (safe.ai)](https://www.safe.ai/research#technical-ml-research)

https://www.anthropic.com/



Goal: Finish AI safety training course, then start doing research with this [SERI ML Alignment Theory Scholars Program (serimats.org)](https://www.serimats.org/). 

"MATS aims to accelerate researchers who will:

- Seek employment at an existing alignment organization (e.g., [Aligned AI](https://buildaligned.ai/), [ALTER](https://alter.org.il/), [Anthropic](https://www.anthropic.com/), [ARC](https://alignmentresearchcenter.org/), [CAIS](https://safe.ai/), [CLR](https://longtermrisk.org/), [Conjecture](https://www.conjecture.dev/), [DeepMind](https://www.deepmind.com/), [Encultured AI](https://www.encultured.ai/), [FAR AI](https://alignmentfund.org/), [MIRI](https://intelligence.org/), [Obelisk](https://astera.org/obelisk/), [OpenAI](https://openai.com/), [Ought](https://ought.org/), [Redwood Research](https://www.redwoodresearch.org/));
    
- Continue to pursue academic alignment research (e.g., [Cambridge](https://www.davidscottkrueger.com/), [UC Berkeley CHAI](https://humancompatible.ai/), [MIT AAG](https://algorithmicalignment.csail.mit.edu/), [NYU ARG](https://wp.nyu.edu/arg/));
    
- Apply to theÂ [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) or [Open Philanthropy](https://www.openphilanthropy.org/how-to-apply-for-funding/) as an [independent researcher](https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency);
    
- Found impactful AI alignment organizations.
    

MATS alumni have gone on to [publish](https://arxiv.org/abs/2302.00805) [safety research](https://arxiv.org/abs/2302.03025), join alignment organizations, including Anthropic and MIRI, and [found an alignment research lab](https://www.lesswrong.com/posts/Q44QjdtKtSoqRKgRe/introducing-leap-labs-an-ai-interpretability-startup). You can read more about MATS alumni [here](https://www.serimats.org/alumni)."



[[AGI]] [[AI]]