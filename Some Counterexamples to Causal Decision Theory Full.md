
The smoking version of Newcomb's problem involves a situation where an agent prefers smoking with no cancer to no smoking, and no smoking is preferred over smoking with cancer. However, some lesion is the common cause of smoking and cancer. Would  it be rational to smoke?

According to evidential decision theory (EDT), the agent's smoking is evidence for the agent of a lesion, hence it would be evidence of cancer, hence you ought not to smoke. Formally: $\Sigma_0^n c(H|A)v(H\wedge A)$ for credence $c$, hypothesis $H$, action $A$ and utility function $v$. Given that one smokes $S$, then the credence that one has a lesion $L$, i.e $c(L|S)$, is higher, hence it judges against smoking. 

According to causal decision theory (CDT), the agents smoking will not cause cancer, hence you ought to smoke. Formally: $\Sigma_0^n c(H)v(H\wedge A)$. Here we only care about the probability of having the lesion $c(L)$, hence it has a lower credence on that outcome. So we hold our current beliefs about the causal structure of the world fixed as we take actions. 

In this example, smoking is the rational choice, since it's the highest on the preference ordering of the agent. Hence CDT makes the correct judgment, but EDT doesn't.

The author follows up with two examples against causal decision theory instead. The first being a scenario where an agent prefers a world without another agent, would be worse off if missing the shot, and thinks that they have good aim. However it's very likely there is a lesion that may cause both a shooting attempt and bad aim at a critical moment. EDT advises correctly to not shoot, since shooting would be evidence of the lesion. CDT advises against this, since shooting does not give one a lesion. 

There are more potential cases, such as time-travel cases under linear (non-branching) time, for instance deciding between saving texts and preventing the fire of Alexandria. Given that you know the fire happen, you would be advised to not prevent the fire since it would be futile. Similar for oracles which reliably predict the future. The author argues that these cases are cases which our theories either doesn't have to apply for whatever reason, or that our intuitions are unreliable and we should let purely theory decide. 

In view of this, the author argues for a way to transform examples against CDT into examples against EDT. What if, in the smoking example, we make the agent believe that the lesion doesn't cause one to get cancer, but rather cause one's lungs to be vulnerable to smoke, such that smoking causes cancer. Now it's irrational to smoke, however CDT implies the agent ought to smoke. 

Here is a general way of constructing a example against CDT from a example against EDT: We take a thought experiment where an agent believes that an common cause causes both action A and undesirable outcome O. Then change it so that the underlying cause doesn't directly cause O, but rather O happens when taking the action A. Now intuitions about the cases have swapped. Since some argue that some of the cases are fantastical or morally loaded, we can now see that these thought experiments are no less fantastical or morally loaded than the one's against EDT. 

Another flaw for EDT is about calculating the probability of taking actions, since $c(H|A) = c(H\wedge A)/c(A)$. Though we could separate conditional probability from this standard definition, say 1 or 0.

In conclusion, the supposed counter-examples make no clear case for either CDT nor EDT. While there might be other arguments, using intuitive judgment of cases has the flaw that any case against CDT can be made against EDT. 

My own view on this topic is to criticize one part of the methodology, namely the method of cases. Intuition about cases squares with results in experimental philosophy (X-phi). Studies in X-phi have shown intuitions to have rather irrational causes and tendencies. Everything from personality traits such as extraversion being a factor in Frankfurt cases about free will, to gender being a factor in intuitions about moral cases such as moral particularism and moral universalism. Gettier cases being partially determined by the ethnicity of the person such as east-Asians being more likely to say Gettier cases are knowledge. Ideological signaling such as bridge-case version of the trolley problem when Buddhists monks are more likely to choose push. Framing effects and order effects on how and when questions are asked change judgments over cases. Many of these irrational causes does not go away after training as a philosopher. We can draw the inductive inference that the intuitions on cases in decision theory will also have a large degree of irrational factors at play. For a large list of examples, check out "philosophy within its proper bounds" by Machery. The general idea is that the method of cases, of testing general theories to our intuitions, is an method not grounded in rational causes, is highly stance-dependent, and is unlikely to yield the kind of results one would want. 

It does appear, given what the author said about cases regarding time-travel that our intuitions may not apply. Van Inwagen argues that agents, or atleast humans, cannot reliably reason about cases in possible worlds far away from the actual world. Although this remains controversial, given that some studies even basic questions about the possibility of which playing cards are left in the card pile given some statements about what has so far been drawn cannot be reliably answered correctly by philosophers. 


