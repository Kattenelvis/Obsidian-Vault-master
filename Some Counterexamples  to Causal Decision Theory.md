> vidential theory by decision courses of action in a range of examples and endorses These "an irrational are fatal problems theory delivers not endorse policy of managing for evidential the news" (Lewis 1981, 5).



A smoking version of Newcomb's problem involves a situation where an agent prefers smoking with no cancer to no smoking, and no smoking is prefered over smoking with cancer. However, some lesion is the common cause of smoking and cancer. Is it rational to smoke?

According to evidential decision theory (EDT), the agents smoking is evidence for the agent of a lesion, hence it would be evidence of cancer, hence you ought not to smoke. Formally: $\Sigma_0^n c(H|A)v(H\wedge A)$ for credence $c$, hypothesis $H$, action $A$ and utility function $v$. Given that one smokes $S$, then the credence that one has a lesion $L$, i.e $c(L|S)$, is higher, hence it judges against smoking. 

According to causal decision theory (CDT), the agents smoking will not cause cancer, hence you ought to smoke. Formally: $\Sigma_0^n c(H)v(H\wedge A)$. Here we only care about the probability of having the lesion $c(L)$, hence it has a lower credence on that outcome. So we hold our current beliefs about the causal structure of the world fixed as we take actions. 

In this example, smoking is the rational choice, since it's the highest on the preference ordering of the agent. 

The author follows up with two examples against causal decision theory instead. The first being a scenario where an agent prefers a world without another agent, would be worse off if missing the shot, and thinks that they have good aim. However it's very likely there is a lesion that may cause both a shooting attempt and bad aim at a critical moment. EDT advises correctly to not shoot, since shooting would be evidence of the lesion. CDT advises against this, since shooting does not give one a lesion. 

There are more potential cases, such as time-travel cases under linear (non-branching) time, for instance deciding between saving texts and preventing the fire of Alexandria. Given that you know the fire happen, you would be advised to not prevent the fire since it would be futile. Similar for oracles which reliably predict the future. The author argues that these cases are cases which our theories either doesn't have to apply for whatever reason, or that our intuitions are unreliable and we should let purely theory decide. They may also be consider to unrealistic in some way, but it's enough that agents believe in them. They may also be considered morally loaded, however that doesn't matter if all we focus is on the preference relations. 

In view of this, the author argues for a way to transform examples against CDT into examples against EDT. What if, in the smoking example, we make the agent believe that the lesion doesn't cause one to get cancer, but rather cause one's lungs to be vulnerable to smoke, such that smoking causes cancer. Now it's irrational to smoke, however CDT implies the agent ought to smoke. 

Here is a general way of constructing a example against CDT from a example against EDT: We take a thought experiment where an agent believes that an common cause causes both action A and undesirable outcome O. Then change it so that the underlying cause doesn't directly cause O, but rather O happens when taking the action A. Now intuitions about the cases have swapped. As a consequence, these thought experiments are no less fantastical or morally loaded than the one's against EDT. 

Another flaw for EDT is about calculating the probability of taking actions, since $c(H|A) = c(H\wedge A)/c(A)$. Though we could separate conditional probability from this standard definition, say 1 or 0.

In conclusion, the supposed counter-examples make no clear case for either CDT nor EDT. While there might be other arguments, using intuitive judgment of cases has the flaw that any case against CDT can be made against EDT. 

My own view on this topic is to criticize one part of the methodology, namely the method of cases. Intuition about cases squares with results in experimental philosophy (X-phi). Studies in X-phi have shown intuitions to have rather irrational causes and tendencies. Everything from personality traits such as extraversion being a factor in Frankfurt cases about free will, to gender being a factor in intuitions about moral cases such as moral particularism and moral universalism. Gettier cases being partially determined by the ethnicity of the person such as east-Asians being more likely to say Gettier cases are knowledge. Ideological signaling such as bridge-case version of the trolley problem when Buddhists are more likely to choose push. A lot of these irrational causes does not go away after training as a philosopher. Even in decision theory there's a large degree of irrational factors at play. For a large list of examples, check out "philosophy within its proper bounds" by Machery. The general idea is that the method of cases, of testing general theories to our intuitions, is an method not grounded in rational causes, is highly stance-dependent, and is unlikely to yield the kind of results one would want. 

It does appear, given what the author said about cases regarding time-travel and such. Van Inwagen argues that agents, atleast humans, cannot reliably reason about cases in possible worlds far away from the actual world, although this remains controversial, given that some studies even basic questions about the possibility of which playing cards are left in the card pile given some statements about what has so far been drawn cannot be reliably answered correctly by philosophers. 





If we were to do A, then P $A\square\to P$ 

$${%
  \Box\kern-1.5pt
  \raise1pt\hbox{$\mathord{\rightarrow}$}}$$

![[Pasted image 20250201170719.png]]

Formally:

Let $H_1,...H_n$ be possible worlds

Formally, Evidential:
![[Pasted image 20250201173406.png]]

But for CDT it's just $c(H)$ instead



![[Pasted image 20250201201444.png]]


2 examples, enough that one succeeds. 

Irrational to shoot, irrational to press the psychopath button. Because it leads one to be worse off than one otherwise would be. 

Causal decision theory implies irrationality in these 2 cases, hence CDT is false.

2 more examples

Some are too science-fictiony or morally loaded (supposidely)
Issue: It just needs to be possible, not feasible or reasonable.

![[Pasted image 20250202123941.png]]






As a consequence, CDT counter-examples are no more science fictional or morally loaded than EDT counter-examples





![[Pasted image 20250202130544.png]]

![[Pasted image 20250202130556.png]]
Disagree here: Sirens example of changing preferences






Ratifiability: If for some action, given that it has been decided, there is no other action b such that b is preferable to a under CDT

![[Pasted image 20250202134040.png]]



![[Pasted image 20250202134357.png]]


Newcomb's firebomb involves two boxes, one with a million dollars, and one with a thousand dollars. With remote buttons to select which option to pick, if the very reliable predictor has predicted that you'll two-box, a firebomb goes off, killing of all money. If one-boxing, you get the million dollars. 

Completeness for CDT isn't upheld in newcomb's firebomb. CDT tells one to two-box, even though it's irrational.

But a ratified version of it still tells one to two-box. 


![[Pasted image 20250202135726.png]]
