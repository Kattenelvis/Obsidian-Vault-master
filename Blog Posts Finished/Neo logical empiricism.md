



This is a start of a project of rehabilitating logical empiricism, which will likely include more thoroughgoing explication and justification of what's said below. I believe I am justified, but I might be missing some crucial, mind-blowing counter-argument to the entire enterprise that I just haven't seen yet. A thoroughgoing rehabilitation of logical empiricism would likely also include a historical-sociological analysis of why it was/is still so widely rejected. At this point, I believe that the large rejection is more strongly based on sociological and other non-rational factors not based on argument. The structure is a point-list without entire justification, as this is more of an outline of a major project I might abandon if I find strong rational counterarguments for logical empiricism where simple modifications don't work.

This post advocates for a revival of logical empiricism that defends what the logical empiricists got right against the cult of Wittgenstein, the cult of Quine and the cult of Sellars. Not to discount the philosophers themselves, but rather said followers who took their arguments against logical empiricism to an extreme. They ruined the reputation of logical empiricism, by straw-manning and overexaggerating the logical consequences of their arguments. It's clear that studying the sociology and history of 1960s-1990s era shows that analytic philosophy was in grips of tarnishing the reputation of the great philosophers before them (including the Vienna circle, the berlin circle, Kraft circle and the Lwow-Warsaw school). This is not to discount that era of philosophy as bad, quite the opposite in fact. However, the dismissing logical empiricism is over. I'm supporting a revival of a (albeit more moderate) logical empiricism, particularly strongly influenced by Carnap.

1. Metaphysics is mostly nonsense, unless it's a part of analyzing a specific theory. "Numbers exist" is meaningless outside of a theory but obviously true or false in some theory T based on whether it includes numbers or not. So for example, numbers obviously exist within Peano Arithmetic, but it's nonsense to say they exist "out there". The indispensability arguments make the inference from mathematical objects in our best physical theories quantum mechanics and general relativity implies that they exist "out there". But this is taking the metaphysical question outside of the theory, and it looses it's meaningfulness this way.  
    
2. Instead of the traditional, strict version of verificationism, we will allow some metaphysical statements to still have meaning ([such as Parfit's view on personal identity](https://www.jstor.org/stable/2184309) since it includes meaningful content regarding phenomenal binding/combination laws) so long as they can be at least in principle theoretically verifiable ([Such as my own post on experimental phenomenal binding)](https://thephilosophyaddict.wordpress.com/2024/01/18/experimental-phenomenal-consciousness-research/). I do not take on a particularly strong verification criterion, I think universal quantification and non-complete verification is fine. In fact, as you'll see in point 10. I argue for a kind of probabilism, rather than complete verification. On that note, I also reject Popper's falsification, as then no theories (except some final theory T, or atleast one which implies all currently doable observation sentences) are considered true.  
    
3. The analytic-synthetic distinction, well atleast 3 of them that Quine addresses, may not exist. Instead I propose of the meaning content of a proposition consisting of a spectrum from less informative to more informative. Even a statement such as "bachelors are unmarried men" may give you some information about how the English language works (and hence not entirely analytic). This works with the previously established verification criterion, since we can verify that this is how that word is used (and increase our probabilistic belief in it). While stricter versions of logical positivism is in threat (which makes a distinction between analytic a priori mathematical statements, synthetic a posteriori empirical statement, and meaningless metaphysical statements) our eased version does not suffer such problems at all.  
    
4. The "Logical" part of logical empiricism: Theories T are formulated in some logical symbols S, language L containing the non-logical symbols, together with some deductive system. An example might be propositional logic with the logical symbols $tex \and\or\rightarrow\leftrightarrow$, non logical symbols p,q,r,... who form the well-formed formulas with a recursive grammar, and some deductive system (say, Hilbert deduction with classical logic) which contain all the inference rules such as law of excluded middle, law of non-contradiction, and modus ponens. A theory T is then a set of any propositions in L such as $ tex \{p\and q\}$. Now for the philosophical part: all scientific theories can be formulated in T ([the syntactic view in the structure of scientific theories](https://seop.illc.uva.nl/entries/structure-scientific-theories/#SynVie), which has gained a resurgence in popularity in recent years).  
    
5. The "Empiricism" part of logical empiricism: Following Carnap's project in the Aufbau, all scientific theories T are reducible to some logical theory and phenomenal predicates. Preferable to use simpler logical systems, such as first-order predicate logic unless something else is necessary. He then went further than I would by minimize the number of phenomenal predicates to only a single one, the "phenomenal similarity" relation R where xRy is true iff x and y are a similar phenomenal experience. In the later part of Aufbau, he tries to rid of even the phenomenal similarity predicate, but I don't think that works. Instead I propose my own system, [formal phenomenology](https://thephilosophyaddict.wordpress.com/2023/11/25/formal-phenomenology/) (or even something like [Barry Smith's formal ontology](https://philpapers.org/archive/SMITBT.pdf) when applied to phenomenal consciousness) should be used instead. Utilizing recent research into the mathematical structures of phenomenal consciousness, it's better to use many phenomenal predicates (hue, brightness, loudness, pitch, smell e.t.c). The Vienna circle did emphasize regions of visual patches, and in formal phenomenology this is formalized within set theory to give us subsets $tex v\subset V$ of the visual field to act as such patches. I still agree with Carnap however in how to in reducing theoretical predicates P to phenomenal predicates R is done via some kind of Ramsey-Lewis method and explicative definitions from conceptual engineering. All observation sentences of all theories O have it such that O is completely reducible via definitions to sets of phenomenal experience and the similarity relation R. This kind of reductionism is in stark contrast with current movements to reduce phenomenal predicates to physical predicates, which I believe is still an important project but which has limitations.  
    
6. Scientific theories are (Hempel-Schaffner)-reducible to each other in a mostly linear ordering. An upper theory $tex T_u$ is Hempel-Schaffner reducible to $tex T_b$ iff $tex T_b \vdash T_u$. This allows for a kind of metaphysical foundationalism (assuming theory reduction terminate at some theory $tex T_b^*$). Such a foundational theory logically implies are observation sentences $tex T_b^* \vdash O$. Also, there is most likely no downward causality. We currently don't have a final theory, so humanities currently best foundation are perhaps the 2 incompatible theories Quantum Field Theory and General Relativity. A final theory will most likely a new theory of Quantum Gravity (and consciousness) at the bottom of the chain to unify them all into a final theory (though we can't be certain it's the final theory, especially if there's underdetermination in its metaphysical implications, in a similar way measurement in quantum mechanics is underdetermined).  
    
7. A commitment to scientific naturalism. Philosophy is not a foundation for science, but evolves simultaneously and symbiotically with it. Philosophy deals with the most abstract parts of scientific theories, description of theories in formal logic, and reduction relations. Philosophy also formalizes the scientific process and formalizes assumptions various theories make. Knowledge is formalized in cognitive science and artificial intelligence, likewise the philosophy of mind but with neuroscience (multiple realizability may be true though, so neuroscience is not enough) and ethics with evolutionary psychology (such as game theoretic cooperative behavior among scarce resources for bodies undergoing homeostasis, reproduction and extropy).  
    
8. Kuhn is mostly not correct, there is algorithmic progress in the philosophy of science. This progress is however more Lakatosian than Popperian. There are no massive paradigms with incommensurability between them, only multiple smaller research projects who are non-incommensurable and who shift in favor not for sociological reasons but for methodological reasons. Ways to formalize the algorithmic progress of science may include Hempel's various [theories of explanation](https://plato.stanford.edu/entries/scientific-explanation/#DNMode), hypothesis testing models ([formal learning theory](https://plato.stanford.edu/entries/learning-formal/)) and such. I do think that the Quine-Duhem thesis still holds, and the new problem of induction, which both allow some degree of underdetermination of scientific theories (we may still select simplest theories, perhaps using a measure such as program length as seen in algorithmic information theory). Scientific discovery can also sometimes be caused by some semi-rational process, however actually applying concepts still requires rationality. The typical example of the discovery of benzene in organic chemistry by dreaming about a [snake biting its own tail](https://en.wikipedia.org/wiki/August_Kekul%C3%A9#Kekul%C3%A9's_dream). The problem is that anyone can dream this, but fail to apply it rationally. At best, it caused an associated thought of the benzene. Application and discovery is still largely algorithmic, though it might be hard to make such an algorithm (if it was easy we'd likely already fully automated science by now).  
    
9. Knowledge can only be gained through the senses and no other way. Each phenomenal region P is in fact a given (and it is not a myth). This can be formalized in formal phenomenology. Phenomenal experience and memory of phenomenal experience and the similarity relation R, alternatively Barry Smith's topological formalization of Husserl's phenomenology, are what basic beliefs consist of. They are, as Sellars points out, atleast somewhat theory laden. As such we instead adopt [Susan Haack's foundherentism](https://www.wiley.com/en-us/Evidence+and+Inquiry%3A+Towards+Reconstruction+in+Epistemology-p-9780631196792) and [Thagard's formalization](https://mitpress.mit.edu/9780262700924/coherence-in-thought-and-action/) of it. This means that our theories also epistemically justify our observations, and not just having observations as basic beliefs which justify theories. This allows  
    
10. The analysis of knowledge is not based on listing sufficient and necessary conditions (justified, true, belief, other criterions) based on thought experiments that lead nowhere and use the now defunct "method of cases" as been shown by experimental philosophy to be way to dependent on non-rational factors such as ethnicity and gender. Instead we propose a belief system informed by rational agent action, utilizing the [Dutch book arguments](https://plato.stanford.edu/entries/dutch-book/) to argue for probabilistic beliefs, and the diachronic Dutch books for updating beliefs with evidence [using Bayes's formula](https://plato.stanford.edu/entries/epistemology-bayesian/). The problem of the priors is best solved with putting priors to objective probabilities (the principal principle) which justifies the convergence theorems (thus giving us a good explanation for why scientists converge on belief over time). Such convergence justifies scientific realism, a kind of optimistic meta-induction (IBE is not needed to justify realism). Such convergence of theories might in turn justify metaphysical realism (assuming a final theory contains external-world predicates).




Potentially, the convergence theorems and the commensurability of scientific theories allows for a scientific realism, which also justifies the existence of the external world. 

The system can be visualised as a cylinder! Rather than a web of belief like Quine or a pyramid like Carnap.

If two towers of theories underdetermine eachother..... big trouble jk jk just pick shortest program length inspired by AIXI which is pareto optimal in all environments. 
![[Underdetermination of final theories.excalidraw]]


Also I'm wondering if the distinction between different higher level theories is even needed. Maybe for pragmatic purposes but in reality it's probably enough to have just one. Although on the other hand, which inference rules we want may be shaped by the higher level theories and observation sentences at those levels. 


