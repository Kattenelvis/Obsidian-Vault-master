**Abstract:** A major issue in contemporary ethical theory to be its reliance on thought experiments and moral intuitions. Moral intuitions are generally correlated by irrational factors such as age, background, ethnicity, personality and so on. Moral intuitions often have a history, not based on justifying true moral norms, but rather based on evolutionary biology (cooperating primates survived and could spread their genes) as well as cultural evolution (certain moral norms spread as societies which adopted them succeeded and out-competed other societies). Studies in moral psychology demonstrates how choices people make in ethical dilemmas are impacted by framing effects and ordering effects. Some of these effects don't necessarily go away with philosophical expertise. The free will debate, which has close links to debates in ethics, is closely related to one's extroversion in the big-5 personality scores, and this bias doesn't go away with philosophical expertise. I propose that ethics should be built on a foundation on rational choice theory and game theory instead, such as Gauthier. The article will consider some issues with Gauthier's approach, and combine it with Roemer's analysis of class based cooperative game theory.

Often times, moral intuitions are invoked via thought experiments. An example of such a thought experiment might be the trolley problem, but the list is very long. I'm not arguing this because I personally lack moral intuitions, I've had them before. When I'm introspecting on my own moral intuitions, I can usually sense a very mild pain sensation from considering some options. Some correlate with a lower level of suffering than others, sometimes even pleasure when an option seems just good. I cannot vow for how others feel about their moral intuitions nor their abilities to introspect, but that's atleast the results of myself. The evolutionary debunking arguments make the case that moral intuitions developed as a way for early primates to avoid disease and facilitate cooperation. Moral intuitions also developed as part of cultural evolution where some societies are better off if they teach individuals to have some moral intuitions. We in the west are generally impacted by a mix of Judeo-Christian intuitions and modern secular moral intuitions. There's probably nothing too much beyond a slight mini-pleasure and mini-suffering sensation when thinking about certain moral norms or thought experiments.

So it's clear that a constructive project is better. Some notion of normativity is not based on these kinds of intuitions, namely rational choice theory, grounded in avoiding sure losses. Sure-loss avoidance as a ground is not an intuition but can be argued for, and I take it by definition that I don't want to take actions that lead to states that are less preferred. By eventually considering the multi-agent case of multiple rational agents, this allows us to build a new ethics from scratch without moral intuitions.

In the study of rational choice theory, there are two main kinds of sure losses. One is money-pumps, and the other is the dutch book arguments. The money-pump argument states the following: Imagine an agent A which have a preference for Mexico over Germany, Germany over Japan, and Japan over Mexico. In this context, whenever the agent is in Mexico and then goes to Germany as that's the agents preference (assuming the price of going there is worth it), then the airplane company could keep extracting money from him indefinitely. While finite memory can prevent this in the short run, backwards induction arguments show that for any finite memory the agent has, it will eventually be money-pumped this way. The dutch book arguments are more complicated, but they argue something similar for upholding the probability axioms and Bayesian updating to evidence, else also be subject to someone extracting wealth from the agent.

Now let's consider game theory as an extension of rational choice theory with multiple agents, specifically the prisoners dilemma. In this situation, two players have to decide without communication and at the same time whether to 'defect' or 'cooperate'. Both cooperate is good for both, the defector is best of when the other defects, both defecting is worse than cooperation for both. This standard game has been analyzed extensively in the literature. In typical one-shot cases it's rational to defect. In multi-shot games it can be rational to defect via backwards induction. However, with trust-building it can be better to cooperate. Cooperators and groups of trustworthy cooperators are better off, after-all. It's worth noting that there are mathematical proofs that rational agents "ought" (rationally) to act in some way, the thought experiment does not base itself on an intuition.

We can also generalize prisoners dilemma into N-player case, also known as the free rider problem. I might consider the probabilities of being a free-rider, it would be good for me after-all. If I asses an unreasonably high probability of not being the free-rider in a way that lower my expected utility, I will start to cooperate with others by not free-riding and encouraging other's to not free ride and encouraging punishment of free-riding. A group of agents that allows free-rider will be worse in the long run than one who doesn't.

Gauthier more or less provides a kind of ethical theory above. What I want to criticize in Gauthier is his focus on considering current power distributions in society. That is to say, the degree to which we consider who gets what in the contract is, unlike in Rawls where it's based on an original position or veil of ignorance, but rather is based on the current position people are in. Different bargaining positions for different individuals. I reject this, as it cannot provide rational support based on the reasons given by coherence arguments. There is no "view from nowhere". Rawls original position then cannot be justified based on intuitions (or specifically in Rawl's case, reflective equilibrium) alone. It has to be argued for regarding equal consideration of others derived from game-theoretic updating to cover all agents as they currently stand. This is fine so far, however it more or less justifies the exact power structures as they are now, even for agents who are less powerful. I think this is false, because it would be in the best self-interest of many people to become more powerful. As a solution to this, we can look at Roemer's cooperative game theory and its application to social classes. For Roemer, people in the working class can cooperate in a way that they form a coalition against the ruling class, and can push forth things that makes them all better of. The Game-Theoretic case for cooperation with people in one's class is strong, and to maximize the size of this coalition against those who are most powerful. Unlike what Ayn Rand thinks, a true rational self-interested agent is a socialist, not a capitalist.

Trust building can also involve a preference update. Preference updates are especially helpful, since they can allow agents who might otherwise be worse off by the adoption of selfish preference relations. Keep in mind, self-interest theory doesn't preclude agents to have altruistic preferences, and encouraging ever altruistic preferences as a form of trust-building between agents is good for every agent involved in the update. This has to be genuine and not just to score points temporarily, else the trust building is jeopardized further. Some may calculate that this is optimal and go with it, but I encourage to go with those who set up dispositions against this. Eventually, we may consider updating our preferences to include all agents in the universe, atleast all the one's with phenomenal experience. It's speculative and goes more into utilitarianistic theorizing.

Consider this inductive argument: In the past, women and black people might've been excluded, but are now included (via suffrage and civil rights movements). Some at this moment of history argue for all animals to be included (vegan movement). We can draw the inductive inference that eventually, all consciousness will be included eventually.

Argument from rationality: Given the uncertainty of what happens in the future, to hedge one's bets it's good to support a social contract which includes all agents. In the future, one may become a simplistic mind. Why not, while one has power, push for a possible world where all lives are counted as part of the social contract? This can be especially interesting if we're considering cases of technological resurrection, cryogenics and certain panpsychist accounts on what happens after death (namely radical consciousness splitting).

In conclusion, be strategically nice.

Citations:

[Extraversion and compatibilist intuitions: a ten-year retrospective and meta-analyses](https://www.tandfonline.com/doi/abs/10.1080/09515089.2019.1572692)

Gauthier, Morals by Agreement

Roemer, A general theory of exploitation and class

SEP articles on [original position](https://plato.stanford.edu/entries/original-position/), [moral psychology](https://plato.stanford.edu/search/r?entry=/entries/moral-psych-emp/&page=1&total_hits=1750&pagesize=10&archive=None&rank=0&query=moral%20psychology), [experimental philosophy](https://plato.stanford.edu/search/r?entry=/entries/experimental-philosophy/&page=1&total_hits=2240&pagesize=10&archive=None&rank=0&query=experimental%20philosophy), [game theory and ethics](https://plato.stanford.edu/search/r?entry=/entries/game-ethics/&page=1&total_hits=1240&pagesize=10&archive=None&rank=0&query=game%20theory%20and%20ethics), [expected utility theory](https://plato.stanford.edu/entries/rationality-normative-utility/), [dutch book arguments](https://plato.stanford.edu/entries/dutch-book/), [preferences](https://plato.stanford.edu/entries/preferences/)