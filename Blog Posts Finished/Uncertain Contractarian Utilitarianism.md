

Outline:

X-phi and moral psychology, evolutionary debunking

Building a new moral philosophy from reasoning principles alone, no intuitions

These principles would be grounded in rational choice theory, which in turn are grounded in avoiding sure losses via Dutch books and money-pumps

This way one can build up a system of rational choice and game-theory optimal solutions in figuring things out.

PA, twin PA, Generalized PA i.e free-rider problem, twin PA.

Making promises where one does an altruistic preference update

Elster's Cooperative Game Theory 




I consider the rotten core of contemporary ethical theory to be its reliance on thought experiments and moral intuitions. I do not consider moral intuitions to hold any sway, and ethics should be built on a foundation on rational choice theory and game theory. The choices of moral intuitions are often highly dependent on the culture one grew up in and even which gender one is. Moral psychology studies how choices people make in ethical dilemmas are impacted by framing effects and ordering effects. These effects don't necessarily go away with philosophical expertise. The free will debate, which has close links to debates in ethics, is closely related to one's extraversion in the big-5 personality scores, and this bias doesn't go away with philosophical expertise. 

What I like about Gauthier is that he rejects moral intuitions. It is ethics done in a way that's rather unconventional. Often times, moral intuitions are invoked via thought experiments. An example of such a thought experiment might be the trolley problem, but the list is very long. I am not born without intuitions, I've had them before. When I'm introspecting on my own moral intuitions, I can usually sense a very mild suffering sensation from considering some options. Some correlate with a lower level of suffering than others, sometimes even pleasure when an option seems just good. I cannot vow for others moral intuitions nor their abilities to introspect, but that's atleast the results of myself. Moral intuitions are merely a way for fitness maximizing. The evolutionary debunking arguments make the case that moral intuitions developed as a way for early primates to avoid disease and facilitate cooperation. Moral intuitions also developed as part of cultural evolution where some societies are better off if they teach individuals to have some moral intuitions. We in the west are generally impacted by a mix of Judeo-Christian intuitions and modern secular intuitions. I think there's nothing too them beyond a slight mini-pleasure and mini-suffering sensation when thinking about them.

What I'd prefer then is a constructive project. I have come to the conclusion that some normativity is fine, namely rational choice theory, grounded in avoiding sure losses. I will not go in-depth on this and why I don't think sure-loss avoidance is an intuition (primarily in this strict definition it's because it's not evaluating any thought experiments). Regardless, this allows us to build a new ethics from scratch without moral intuitions. Whether this counts as 'ethics' in the traditional sense is irrelevant. 

First, let's consider the prisoners dilemma. In this situation, two players have to decide without communication and at the same time whether to 'defect' or 'cooperate'. Both cooperate is good for both, the defector is best of when the other defects, both defecting is worse than cooperation for both. This standard game has been analyzed extensively in the literature. In typical one-shot cases it's rational to defect. In multi-shot games it can be rational to defect via backwards induction. However, with trust-building it can be better to cooperate. Cooperators and groups of trustworthy cooperators are better off, after-all. It's worth noting that there are mathematical proofs that rational agents "ought" (rationally) to act in some way, the thought experiment does not base itself on an intuition.

Trust building can also involve a preference update. Preference updates are especially helpful, since they can allow agents who might otherwise be worse off by the adoption of selfish preference relations. Keep in mind, self-interest theory doesn't preclude agents to have altruistic preferences, and encouraging ever altruistic preferences as a form of trust-building between agents is good for every agent involved in the update. This has to be genuine and not just to score points temporarily, else the trust building is jeopardized further. Some may calculate that this is optimal and go with it, but I encourage to go with those who set up dispositions against this. 

We can also generalize prisoners dilemma into N-player case, also known as the free rider problem. I might consider the probabilities of being a free-rider, it would be good for me after-all. If I asses an unreasonably high probability of not being the free-rider in a way that lower my expected utility, I will start to cooperate with others by not free-riding and encouraging other's to not free ride and encouraging punishment of free-riding. A group of agents that allows free-rider will be worse in the long run than one who doesn't.

We can look at Elster's views on cooperative game theory and class struggle. For Elster, people in the working class can cooperate in a way that they form a coalition against the ruling class, and can push forth things that makes them all better of. The Game-Theoretic case for cooperation with people in one's class is strong, and to maximize the size of this coalition against those who are most powerful. 

Eventually, we may consider updating our preferences to include all agents in the universe, atleast all the one's with phenomenal experience. It's speculative and goes more into utilitarianist theorizing, something which I was saddened to have to give up on, I used to have strong utilitarian intuitions before my systematized ethical theory. Social Choice Theory is another great theory to go over that won't be covered here. 

Arguments for extending the social contract to all agents.

We have agents $A_1,...A_N$ for a size $N$ population. We take it that only some subset $A_1,...A_i$ are part of the social contract, for $i<N$. 

Inductive argument: In the past, women and black people might've been excluded, but are now included (via suffrage and civil rights movements). Some at this moment of history argue for all animals to be included (vegan movement). We can draw the inductive inference that eventually, all consciousness will be included eventually. This is similar to an argument for utilitarianism, namely the historical correctness argument (or whatever it was called TODO: look it up). It's not a convincing argument. 

Argument from rationality: Given the uncertainty of what happens in the future, to hedge one's bets it's good to support a social contract which includes all agents. In the future, one may become a simplistic mind. Why not, while one has power, push for a possible world where all lives are counted as part of the social contract?

Different bargaining positions for different individuals. Rawls argued for a original position to go out for. I reject this, it cannot provide rational support based on the reasons given by coherence arguments. There is no "view from nowhere". 

Rawls original position then cannot be justified based on intuitions or reflective equilibrium alone. It has to be argued for regarding equal consideration of others derived from game-theoretic updating to cover all agents. So we re-capture quite a lot of ethics in this way, without appeal to intuitions.

In conclusion, be nice. 


What I like about Gauthier is that he rejects moral intuitions. It is ethics done in a way that's rather unconventional.


[Harsanyi.pdf](https://home.sandiego.edu/~baber/gender/Harsanyi.pdf)

Harsanyi was claimed by Gauthier to be a contractarian utilitarian of sorts. Rawlsian ish.

Gauthier is closer to what I'm for though, given the coherence argument foundation.

Cooperate if everyone have the same preferences

Psychological hedonism is true, i.e everyone prefers more hedonic states over less hedonic states

Therefore, cooperate with everyone.



Social contract


Another assumption might be the existence of mental states, which some eliminativists have said to be eliminable from our ontology. For any theory of practical rationality, actions is a requirement in such an ontology, lest it be impossible. 

Beliefs could in principle be betting predispositions, which in turn could in principle be determined empirically. Preferences could, given psychological hedonism, just be the amount of pleasure and suffering in some conscious state, maybe reducible/identical/emergent from/whatever else from physical (brain) states. Actions could likewise have to do with the sensation of taking actions, perhaps reducible/identical/whatever to physical (brain) states.

One potential "problem" with this view, is that we don't get a normatively guiding theory of practical rationality, we merely get a theory of psychology. This is compatible with eliminativism. 

I might be completely wrong here lmao
> This may inspire future experiments to characterize how the nervous system implements such a prior.
[Decision Theory What Should the nervous system do.pdf](file:///C:/Users/Katte/Documents/Academic/Philosophy/Decision%20Theory%20What%20Should%20the%20nervous%20system%20do.pdf)


Money pump --> Transitive preferences
Dutch books --> Completeness

Transitive preferences and complete preferences imply linear preference relation/utlity function

Dutch books avoids sure losses (what kind of preference relation is needed? A utility  should be sufficient, but is it necessary?). Impossible or atleast unfeasible in reality to accomplish (AIXI uncomputability). So we adopt hueristics and bounded rationality, but try to optimize first-order rationality as much as possible with this second-order rationality. 

Different bargaining positions for different individuals. Rawls argued for a original position to go out for. I reject this, it cannot provide rational support based on the reasons given by coherence arguments. There is no "view from nowhere". 

Persons-affecting view yes, ideal observer theory no. Moral realism no, there are no stance-independent and necessary moral facts.

A social contract is formed between different unequal agents. Support a generally equal system as cooperatively optimal between agents. Rule-utilitarianism could be established, human rights, norms, virtues etc. Social choice theory, game theory, optimization theory, economics etc can be used for finding optimum positions.

Thus we establish a sovereign, based on an improper subset (could be optimal for all to have the entire population in some cases). Anarchism is generally sub-optimal. We care less for future generations, so no intergenerational welfare economics although tbh it can be used to model a long lived population. Population ethics consequences in general? It's not good for anyone before they're born to be born. What about anti-natalist views where it's bad for someone to be born? What about transhumanism? What about fair cake division? So many questions!



![[Pasted image 20250124213120.png]]





Gauthier set of essays.

Gauther grounds moral claims in rationality. Rejects moral intuitions and reflective equilibrium. 

Not all humans, animals nor future generations are part of the contract according to Gauther

Gauther's view has no veil of ignorance nor original position in general. We're barganing from our current position.


Perhaps 

Gauthier book

![[Pasted image 20250113215340.png]]




Arguments for extending the social contract to all agents.


We have agents $A_1,...A_N$ for a size $N$ population. We take it that only some subset $A_1,...A_i$ are part of the social contract, for $i<N$. 

Inductive argument: In the past, women and black people might've been excluded, but are now included (via suffrage and civil rights movements). Some at this moment of history argue for all animals to be included (vegan movement). We can draw the inductive inference that eventually, all consciousness will be included eventually.

Argument from rationality: Given the uncertainty of what happens in the future, to hedge one's bets it's good to support a social contract which includes all agents. In the future, one may become a simplistic mind. Why not, while one has power, push for a possible world where all lives are counted as part of the social contract?







Issues of Gauthier in Parfit


Parfit vs Gauthier?