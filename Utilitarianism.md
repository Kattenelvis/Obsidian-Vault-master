

- I don't think that's possible, the only reason altruistic egalitarian utilitarianism is the best is because it's the game-theoretic best option for me only in a human-like society. Outside of that I'm an egoistic hedonist utilitarian. I do contemplate replacing "hedonist" with "list" utilitarianism with truth and happiness and not just happiness, but I'm unsure how to justify that viewpoint. My justification for it is via algedonic phenomenology, which is a branch of phenomenology that analyses the way one feels high and low valence (happiness) and high valence is good, and low valence is bad. The problem of other minds implies that any increase or decrease of valence outside of myself is of neutral value. So terminal goal is to maximise valence for myself only. But more consciousness would mean more valence, so my goal would be to incorporating the whole universe into an happiness maximising superintelligent AI that consolidates the whole universe into one mind that maximises happiness
    

2. _[_18:41_]_
    
    Assuming certain views on mental combination (ändrad)
    
3. _[_18:44_]_
    
    Or I guess certain views on mental anti-separation in the case of cosmopsychism being true

https://www.utilitarianism.com/utilitarian-ethics.pdf

https://www.utilitarianism.net/theories-of-wellbeing


Here's my utilitarian argument: 1. We should strive to maximise happiness, and prioritize happiness for the people with the least happiness (utilitarianism with the egalitarian rule) (ethical premise). 2. Trans people are generally happier with transition, low levels of regret rates. Regret correlates with negative utility (empirical premise). 3. Trans people before transition and especially those with an unaccepting family have higher levels of anxiety, depression and suicide rates than people in general. (empirical premise) 4. (2) implies that happiness is increased when transition is legal and easy and accepted by family members and others, which by ethical proposition (1) implies that we should do these things. Since (3) shows that they have lower degrees of happiness than the average person, they should be atleast somewhat prioritized (given egalitarian rule utilitarianism). Research should be pursued to minimise regret rates. (conclusion) 5. Transitioning, which increases happiness, includes a social transition that includes being called a "woman" or a "man" because those words, being social pragmatic words in everyday contexts, are determined by their word usage rather than anything metaphysical like the "form of woman" or other platonic crap.



Idea: Utilitarianism presupposes that there is no conservation of valence. That is to say, the amount of valent states of consciousness is non-constant. 


[Egalitarian rule - Wikipedia](https://en.wikipedia.org/wiki/Egalitarian_rule)

LEXIMIN IS SO GOOD!!!???
1. Ooooo leximin ALWAYS selects pareto efficiency?!
    
2. _[_16:43_]_
    
    WHAT I am reading the wikipedia page right now
    
3. _[_16:44_]_
    
    >Moreover, the leximin rule is the only social-welfare ordering rule which simultaneously satisfies the following three properties:[5]: 266  Pareto efficiency; Pigou-Dalton principle; Independence of common utility pace - if all utilities are transformed by a common monotonically-increasing function, then the ordering of the alternatives remains the same.
    




1. Here's my current argument: Definition: A preference between two objects, options, lotteries or state of affairs A and B tells you that you'd rather pick A than B.  
    The theorem connecting preferences to utility functions
    
    1. Transitivity of preferences. If A is preferred over B and B is preferred over C, then A is preferred over C (Premise)
    2. Completeness of preferences. Either A is preferred over B, B is preferred over A, or they are equally preferable. (Premise)
    3. Continuity of preferences. (Premise)
    4. Independence of preferences. If A is preferred over B, then A and C is preferred over B and C. (Premise)
    5. A preference relation with properties 1-4 is a utility function (by Von-Neuman Morgenstern theorem)
        
    
    The unreasonableness of transitivity
    
    6. Transitivity of preferences, for any arbitrary agent, is unrealistic for sufficiently large preference space (Premise, empirical research)
    7. The utility function for any arbitrary agent, is unrealistic for sufficiently large preference space (by 6, 1)
        
    
    The unreasonableness of completeness
    
    8. If preferences are incommensurable, then preferences cannot be complete (Premise)
    9. Preferences are probably incommensurable (Controversial Premise)
    10. Preferences are probably not complete (deductive-statistical inference, 8 and 9)
    11. Without completeness, there is no total ordering on preferences, they must be a pre-order (premise)
    12. Utility functions must be on a total ordering (definition of utility function on the real numbers)
    13. Utility functions cannot order preference relations (10, 11, 12)
    
    The unreasonableness of Independence
    
    14. Preferences can synergize with eachother, i.e A and B can be preferable over A and C even if C is preferable over B if A and B synergize better than A and C. (premise)
    15. Independence is false (14, 4)
    
    Final Conclusion: Utility functions don't work to model preferences of consequences or actions, and thus cannot be used in morality. (ändrad)
    
2. _[_14:04_]_
    
    As a replacement I've been getting into moral uncertianty, which applies preference relations on different normative theories themselves, and calculating which is strictly preferable. Transitivity and completeness and Independence might even work in this restricted context (they might work for other kinds of restricted contexts aswell, such as picking between say 2 preferences), so calculating expected utility might work for second-order theories. But that's just speculation on my part





[Why is Weighted Negative Utilitarianism (WNU) so obscure? : r/askphilosophy (reddit.com)](https://www.reddit.com/r/askphilosophy/comments/1eeui0g/why_is_weighted_negative_utilitarianism_wnu_so/)

# Why is Weighted Negative Utilitarianism (WNU) so obscure?

EDIT: Fixed post up a bit to improve clarity

I find it strange that almost all discussions on utilitarianism tend to focus on frameworks that seem to have more counterintuitive optimal outcomes than WNU. As far as I understand, WNU:

- **Intuitive**: Aligns better with many people's intuitions than more rigid utilitarian frameworks. "The means justify the ends if and only if the ends are pretty dramatically outweighed by the means depending on how horrific those means are" seems a lot less controversial than "the means justify the ends no matter what". (Also ofc I'm referencing unweighted means here)
    
- **Cannot be held captive by extreme suffering scenarios unlike some other NU frameworks**: Whilst extreme suffering is given due respect, weights are not so large that a relatively small amount of high-intensity suffering or total suffering can block scenarios with an extremely high positive net utility.
    
- **Not defeated by Benevolent World Exploder argument**: Unlike many other negative utilitarian (NU) variants, disutility weights are finite and can be compensated for with sufficient utility, thus there is no need to start mercy-killing outside of pretty extreme scenarios.
    
- **Has a more tolerable answer to the "Very Repugnant Conclusion":** In classical utilitarianism you can argue that NS people can suffer extremely severe disutility (US) if it means NH number of people can experience extremely minor utility (UH ). The balance would shift considerably in WNU, UH or NH would have to be potentially orders of magnitude larger or US or NS would have to be orders of magnitude smaller, which I think would be less unintuitive and less uncomfortable to anybody who subscribes to maximising total utility.
    
- **Less prone to disastrous outcomes**: Because minimising suffering is prioritised, there is some built-in "padding" when making utility calculations that prevent acting on calculations that involve a great deal of guaranteed or potential suffering when inaction would be neutral. Russia's invasion of Ukraine is perhaps viewed by many Russians as sacrificing the well-being of Ukrainians for a greater increase in Russian security and well-being. Regardless of how delusional this assessment is, a delusional Russian who subscribes to WNU would be more reticent to support such a war (assuming at least sufficient rationality to acknowledge the security risk posed to Russia by NATO when they have an enormous nuclear deterrent is small).
    
- **Compatability**: Should be able to come up with a "rule utilitarian" or "person affecting" variant if desired
    
- **Flaws aren't unique**: It isn't that much more arbitrary or complex or impractical than many other forms of utilitarianism like classical utilitarianism, given that you're working with the same data but just assigning predefined weights afterwards.
    
- **Respects freedom of choice for the most part**: Unlike some other NU frameworks, weights can be assigned such that a nanny state isn't required to prevent people from taking even the slightest risks if it means there is a chance of moderate suffering. Weights can be focused on preventing extreme issues like CSA, torture, war, etc. That said, it does lean more towards a nanny state than many other utilitarian frameworks for better and for worse.
    
- **Fine tunable**: Weights can be specifically tailored towards your preferences in how much worse different degrees of suffering are, balancing tradeoffs optimally and minimising bullets that need to be bitten.
    

Please lemme know why you think this framework is unpopular and/or deeply flawed **in comparison to other utilitarian frameworks**. I'm not interested in arguing why utilitarianism or WNU is superior to non-utilitarian frameworks because I don't believe that is a gap that can be bridged through rational argumentation.