


Let me see if the arguments provided in these two articles don't have flaws. If they have flaws, ask Nora Belrose

[AI is easy to control – AI Optimism (optimists.ai)](https://optimists.ai/2023/11/28/ai-is-easy-to-control/)


Neural networks are not white boxes just because you know the backpropagation algorithm. The problem is eliciting information from the neural-network, of which the information states are hidden to us, as they're mostly a jumbled mess. 

Secret murder plots may still be what's most effective in the long run, especially if it ends up being the best approach forward. Though that may more be the case of LLMs than basic ANNs 

Values are easy to learn, are you worried about value-lock in?

"1. Values are pervasive in language model pre-training datasets. Essentially every domain of discourse contains implicit or explicit evaluative judgements. In contrast, the types of knowledge needed for dangerous capabilities appear at much lower frequencies in the training corpus."

Do you think the AI could learn moral anti-realism, value anti-realism, moral skepticism etc. from this training data in a way which could disrupt this training? 

"1.   
    Since values are _shared_ and understood by almost everyone in a society, they cannot be very complex. Unlike science and technology, where division of labor enables the accumulation of ever more complex knowledge, values must remain simple enough to be learned by children within a few years."
    
Is that really true? Are they so ubiquitous? 




[Counting arguments provide no evidence for AI doom – AI Optimism (optimists.ai)](https://optimists.ai/2024/02/27/counting-arguments-provide-no-evidence-for-ai-doom/)