[A Minimaximalist Approach to Reinforcement Learning from Human Feedback (arxiv.org)](https://arxiv.org/pdf/2401.04056.pdf)

"First, assuming an underlying reward function exists is equivalent to assuming that there exists a total order over agent behavior. This means that there are no intransitivities in rater preferences (i.e. A ≻ B, B ≻ C ⇒ A ≻ C), which contradicts what psychology tells us about actual human decision making (Tversky, 1969; Gardner, 1970). Even if one believes an individual person’s preferences are transitive, when aggregated across a population of raters as is necessary at scale, transitivity is unlikely to be satisfied (May, 1954). Second, given the inherent stochasticity of human preferences (Agranov & Ortoleva, 2017), one often learns a reward model that leads to a collapse in generation diversity."



[1706.03741.pdf (arxiv.org)](https://arxiv.org/pdf/1706.03741.pdf)