https://www.lesswrong.com/posts/3RSq3bfnzuL3sp46J/acausal-normalcy

> **Summary:** Having thought a bunch about acausal trade — and proven some theorems relevant to its feasibility — I believe there _do not exist_ powerful information hazards about it that stand up to clear and circumspect reasoning about the topic.  I say this to be comforting rather than dismissive; if it sounds dismissive, I apologize.


## See also

- ["AI deterrence"](http://aibeliefs.blogspot.com/2007/11/non-technical-introduction-to-ai.html?a=1)
- ["The AI in a box boxes you"](https://www.lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you)
- [A story](https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/) that shows acausal trade in action.
- [Scott Alexander](http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) explains Acausal Trade. (Most of that article is tongue-in-cheek, however.)
- "[Hail Mary, Value Porosity, and Utility Diversification](http://www.nickbostrom.com/papers/porosity.pdf)," Nick Bostrom, the first paper from academia to rely on the concept of acausal trade.
- [Towards an idealized decision theory](http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf), by Nate Soares and Benja Fallenstein discusses acausal interaction scenarios that shed light on new directions in decision theory.
- [Program Equilibrium](https://ie.technion.ac.il/~moshet/progeqnote4.pdf), by Moshe Tennenholtz. In: Games and Economic Behavior.
- [Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic](https://arxiv.org/abs/1401.5577), by Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire and Eliezer Yudkowsky
- [Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents](https://arxiv.org/abs/1602.04184), by Andrew Critch
- [Robust Program Equilibrium](https://link.springer.com/article/10.1007/s11238-018-9679-3), by Caspar Oesterheld. In: Theory and Decision.
- [Multiverse-wide Cooperation via Correlated Decision Making](https://foundational-research.org/multiverse-wide-cooperation-via-correlated-decision-making/), by Caspar Oesterheld

## References

- [Hofstadter's Superrationality essays, published in _Metamagical Themas_](http://www.gwern.net/docs/1985-hofstadter) ([LW discussion](https://www.lesswrong.com/lw/bxi/hofstadters_superrationality/))
- Jaan Tallinn, [Why Now? A Quest in Metaphysics](https://www.youtube.com/watch?v=29AgSo6KOtI).
- [Gary Drescher](https://wiki.lesswrong.com/wiki/Gary_Drescher), _Good and Real_, MIT Press, 1996.
- [Functional Decision Theory](https://arxiv.org/abs/1710.05060)



From here: https://www.reddit.com/r/SneerClub/comments/m24vui/am_i_just_stupid_or_is_lws_acausal_trade_idea/
(Despite this subreddit opposing lesswrong, this comment is very nuanced and well thought-out)

(I go into some detail about acausal decision theory here, maybe I should start with a **WARNING,** even on [r/sneerclub](https://www.reddit.com/r/sneerclub/))

[According to the official history](https://www.lesswrong.com/tag/acausal-trade), it all started with an attempt to beat the Prisoner's Dilemma, a classic scenario of game theory. Ordinary self-interest says you should "defect", but if only both players could "cooperate", then they could both have a higher payoff. Douglas Hofstadter dubbed this "irrational" decision to cooperate _superrationality_.

Then someone called Gary Drescher published a book _justifying_ superrationality, in the case where the two players are computer programs running the same source code. If you're in a Prisoner's Dilemma with an exact copy of yourself, then you might reason that you can rationally choose to cooperate, since your other self will do the same thing. But you don't actually control your other self, so what is the exact justification for this confidence? Drescher apparently introduced the word "acausal" to the discussion.

Drescher, incidentally, is an enigmatic figure to me. There's very little information about him. His book "Good and Real" came out in 2006, the year in which the group blog "Overcoming Bias" was also launched ("Less Wrong" emerged from "Overcoming Bias" three years later). I have not read the book, but it contains acausal decision theory _and_ a defense of the many-worlds interpretation.

Another "application" of this thinking is to resolve Newcomb's paradox, which is a little like being in a Prisoner's Dilemma, not with a copy of yourself, but with a superintelligence which will defect if you defect, and cooperate if you cooperate. I won't go into the details, but you have a choice between being greedy and being restrained, and the superintelligence has promised you a big reward if you are restrained and a small reward if you are greedy. And the paradox is that the superintelligence already predicted your choice and determined the size of the reward. Ordinary causal thinking then says, you may as well be greedy and grab everything, because the reward is already set; but if you do that, you will be retro-causing yourself to have a small reward. How do you rationally justify being restrained?

The answer is an extended version of Drescher superrationality. The superintelligence is not a copy of you, but its decision is a copy of your decision. You should be restrained now, _because_ that implies the superintelligence will have modeled you as restrained, and left the big reward. This violates the usual dictum that the future cannot influence the past, so, "acausal". Perhaps it would have been better to speak of logical causality or atemporal causality, but this is the dominant temrinology now.

In any case, there has been a progressive generalization of the concept, to agents that are only vaguely similar, agents located in different universes, populations of agents in different universes reasoning their way to a collective equilibrium, and even quasi-theologies like, all possible godlike superintelligences that rule their respective universes, arriving at an acausal decision equilibrium among themselves.

Recounting this history leads me to think that, from the perspective of history of ideas, Roko's basilisk should be regarded as an episode in the history of decision theory, that can usefully be placed alongside Newcomb's paradox and Hofstadter's approach to the Prisoner's Dilemma. Its notoriety as an object of fear or derision obscures the fact that it is also a thought experiment for decision theorists.

People get tripped up on the fact that it's a scenario in which there is a causal link as well as an acausal link - the possible AI is in our future, that's the causal link - and the acausal part is overlooked in favor of the grand guignol of "punishment by the robot god". But viewed abstractly, it's just a variation on Newcomb's paradox, but with the superintelligence that models your decision in the future, rather than in the past.

Returning to the post above: is "acausal trade" really just a kind of daydream, an exercise in "trading" with imaginary friends and enemies? Certainly the multiverse version seems problematic on multiple levels. We don't even know that other universes exist, so how can we know that we're trading with them? And even if they do exist, I would question whether any relationship possessing the mutuality implied by the designation of _trade,_ is actually possible among them. If Chuang-tzu does something for the sake of the butterfly, and the butterfly does something for the sake of Chuang-tzu, is that a "trade", or just a fortuitously consistent _folie a deux_?

At a mundane level, a sneer may be enough to chase away the basilisk doldrums. But I suppose the idea needs a more formal way of being countered too. So let me propose a _hypothesis of omniversal autarky_: That the set of superintelligences which attempt acausal trade has measure zero, because it quickly becomes clear to a hyper-rational being that you should only care about things you can influence causally. I can't prove it, but neither can the philosophes of acausality disprove it. Let them try to do so, and until they do so, feel free to focus on this universe alone.