


[How To Get Into Independent Research On Alignment/Agency - AI Alignment Forum](https://www.alignmentforum.org/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency)
## Getting Paid

"Right now, the best grantmaker in this space is the [Long-Term Future Fund (LTFF)](https://funds.effectivealtruism.org/funds/far-future). There are [other options](https://www.alignmentforum.org/posts/9DenhM8deDziHiWZw/what-funding-sources-exist-for-technical-ai-safety-research?commentId=oR6KytzH9TBoEDS4n), but none are quite as good a fit for the sort of work we’re talking about here.

I’ve received a few LTFF grants myself and know some of the people involved in the grantmaking decisions, so I’ll give some thoughts on the most important things you’ll need in order to get paid. Bear in mind that this is inherently speculative and not endorsed by anyone at LTFF. I’d also recommend looking at LTFF’s [past](https://funds.effectivealtruism.org/funds/far-future#payout-reports) [grants](https://funds.effectivealtruism.org/funds/payouts/may-2021-long-term-future-fund-grants) to get a more direct idea of what kinds of things they fund."



Hamming question
"Over on the other side of the dining hall was a chemistry table. I had worked with one of the fellows, Dave McCall; furthermore he was courting our secretary at the time. I went over and said, "Do you mind if I join you?" They can't say no, so I started eating with them for a while. And I started asking, "What are the important problems of your field?" And after a week or so, "What important problems are you working on?" And after some more time I came in one day and said, "If what you are doing is not important, and if you don't think it is going to lead to something important, why are you at Bell Labs working on it?" I wasn't welcomed after that; I had to find somebody else to eat with!"


## Technical background 
"This is one reason why it helps to have a broad technical background: the more frames and tools you have to draw on, the more likely you’ll find a novel and promising combination to apply to the most important problems in the field. (Or, just as good: the more frames and tools you have to draw on, the more likely you’ll notice that one of the most important problems has been overlooked.)

Flip side of this: if you have a novel-seeming idea which involves the same kinds of frames and tools which most people in alignment have (i.e. programming expertise, some ML experience, reading [Astral Codex Ten](https://astralcodexten.substack.com/)) then do write it up, but don’t be surprised if it’s already been done.

If you read through some existing alignment work, and the strategy seems obviously wrong to you in a way which would not be obvious to the median LessWrong user, then that’s a very promising sign."

Philosophy gang gang! And also mathematics. Still a lot of overlap with many less-wrong users, so keep doing research on democracy aswell (social choice theory).


## Legibility


Part of getting a grant is not just having a good plan and the skills to execute it, but to make your plan and skills legible to the people reviewing the grant.

Here’s (my summary of) a rough model from Oli, who’s one of the fund managers for LTFF. In order to get a grant for alignment research, usually someone needs to do _one_ of these three:

1. Write a grant application which clearly signals that they understand the alignment problem and have a non-bullshitted research strategy. (This is rare/difficult.)
2. Have a reference from someone the fund managers know and trust (i.e. the existing alignment research community).
3. Have some visible online material which clearly signals that they understand the alignment problem and have a non-bullshitted research strategy. (LessWrong posts/comments are a central example.)

As a new entrant to the field, I expect that option #3 is probably your main path. Write up not just your research strategy, but the intuitions, models and arguments behind that strategy. Give examples. Explain what you consider the key problems, why those problems seem central, and the frames and generators behind that reasoning. Again, give examples. Explain conjectures or tools you think are relevant, ideally with examples. If you’re on the theory side, sketch potential empirical tests; if on the empirical side, sketch the conceptual theory behind the ideas. And include examples. Explain your vision of success, and expected applications of your research (if it succeeds). At all stages, focus on giving accessible, intuitive explanations and **lots of examples**; even people who have lots of technical background will often skip over sections with just dense math, and not everyone has the _same_ technical background as you. And [put the examples at the beginnings of the posts, before the abstract/general explanations](https://www.alignmentforum.org/posts/CD2kRisJcdBRLhrC5/the-power-to-teach-concepts-better#Teach_With_Examples_First).