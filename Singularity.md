[1202.6177.pdf (arxiv.org)](https://arxiv.org/pdf/1202.6177.pdf)

Hutter singularity

He assumes the physical church-turing thesis.. interesting!

"Chalmers [Cha10] discusses various potential obstacles for a singularity to emerge. He classifies them into structural obstacles (limits in intelligence space, failure to takeoff, diminishing returns, local maxima) and manifestation obstacles (disasters, disinclination, active prevention) and correlation obstacles. For instance, self-destruction or a natural catastrophe might wipe out the human race [BC08]."

"But even if so, today’s computers are so far away from these limits, that converting our planet into computronium would still result in a vastly different vorld, which is considered a reasonable approximation to a true singularity."

"First, the hardware (computers) for increasing comp must be manufactured somehow. As already today, this will be done by (real) machines/robots in fac tories. Insiders will provide blue-prints to produce better computers and better machines that themselves produce better computers and better machines ad infini tum at an accelerated pace. Later I will explain why insiders desire more comp. Non-accelerated real human (outsiders) will play a diminishing role in this process due to their cognitive and speed limitations. Quickly they will only be able to pas sively observe some massive but incomprehensible transformation of matter going on."

"Imagine an inward explosion, where a fixed amount of matter is transformed into increasingly efficient computers until it becomes computronium. The virtual society like a well-functioning real society will likely evolve and progress, or at least change. Soon the speed of their affairs will make them beyond comprehension for the out siders. For a while, outsiders may be able to make records and analyze them in slow motion with an increasing lag. Ultimately the outsiders’ recording technology will 6 not be sufficient anymore, but some coarse statistical or thermodynamical proper ties could still be monitored, which besides other things may indicate an upcoming physical singularity. I doubt that the outsiders will be able to link what is going on with intelligence or a technological singularity anymore."


"Insiders may decide to interact with outsiders in slow motion and feed them with pieces of information at the maximal digestible rate, but even with direct brain-computer interfaces, the cognitive capacity of a human brain is bounded and cannot explode. A technologically augmented brain may explode, but what would explode is the increasingly dominant artificial part, rendering the biological brain eventually superfluous — a gradual way of getting sucked into the inside world. For this reason, also intelligence amplification by human-computer interfaces are only temporarily viable before they either break down or the extended human becomes effectively virtual."

"There is lots of motivation to compress information (save memory, extract regularities, and others), but it is well-known [LV08] that maximally compressed information is indistinguishable from random noise." ????? This is intruiging


"Let us now consider outward explosion, where an increasing amount of matter is transformed into computers of fixed efficiency (fixed comp per unit time/space/energy). Outsiders will soon get into resource competition with the expanding computer world, and being inferior to the virtual intelligences, probably only have the option to flee. This might work for a while, but soon the expansion rate of the virtual world should become so large, theoretically only bounded by the speed of light, that escape becomes impossible, ending or converting the outsiders’ existence. So while an inward explosion is interesting, an outward explosion will be a threat to outsiders. In both cases, outsiders will observe a speedup of cognitive processes and possibly an increase of intelligence up to a certain point. In neither case will outsiders be able to witness a true intelligence singularity."

"Currently the technological explosion is both inward and outward (more and faster computers). Their relative speed in the future will depend on external con straints. Inward explosion will stop when computronium is reached. Outward ex plosion will stop when all accessible convertible matter has been used up (all on earth, or in our galaxy, or in our universe)."


"There have been numerous attempts to define intelligence; see e.g. [LH07a] for a collection of 70+ definitions from the philosophy, psychology, and AI literature, by individual researchers as well as collective attempts."


"What activities could be regarded as or are positively correlated with intelligence? Self-preservation? Self-replication? Spreading? Creating faster/better/higher intelligences? Learning as much as possible? Understanding the universe? Maximizing power over men and/or organizations? Transformation of matter (into computronium?)? Maximum self-sufficiency? The search for the meaning of life?"
Idea: The conflict between converging instrumental rationality and the orthogonality thesis

""In [LH07b] intelligence has been defined as the ability to achieve goals in a wide range of environments. It has been argued that this is a very suitable character ization, implicitly capturing most, if not all traits of rational intelligence, such as reasoning, creativity, generalization, pattern recognition, problem solving, mem orization, planning, learning, self-preservation, and many others. Furthermore, this definition has been rigorously formalized in mathematical terms. It is non anthropocentric, wide-range, general, unbiased, fundamental, objective, complete, and universal. It is the most comprehensive formal definition of intelligence so far. It assigns a real number Υ between zero and one to every agent, namely the to-be-expected performance averaged over all environments/problems the agent po tentially has to deal with, with an Ockham’s razor inspired prior weight for each environment. Furthermore there is a maximally intelligent agent, called AIXI, w.r.t. this measure. The precise formal definitions and details can be found in [LH07b], but do not matter for our purpose. This paper also contains a comprehensive justi f ication and defense of this approach.""





A criticism of the singularity:
[THOATS-5.pdf (philpapers.org)](https://philpapers.org/archive/THOATS-5.pdf)


"Even if there is nothing special about the problem of improving artificial agents, could there be something special about the researchers we put this problem to? Perhaps dimin ishing research productivity afflicts human researchers, but not artificial agents. There is a sliver of truth to this objection: one cause of diminishing research productivity is the difficultyofmaintaininglargeknowledgestocks(Jones2009),aproblematwhichartificial agents excel. However, the underlying problem of fishing-out is a feature of problems, not agents, and as suchit cannot be eliminated by enlisting artificial agents as researchers. After a while, any sensible investigatory process will have made more than its share of the easiest discoveries, and subsequent discoveries will become harder. Past that point, research productivity will diminish."


