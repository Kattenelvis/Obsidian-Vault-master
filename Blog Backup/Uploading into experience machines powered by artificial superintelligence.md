**One persons utopia is someone else's dystopia, instead, let people create their own personal utopias.**

In this article I will argue for using friendly artificial super-intelligence (FASI) to set up conditions for which it is good to extract the environment in order to build Dyson spheres and von-Neumann probes whose energy will be utilized by mind-uploaded conscious minds to produce virtual worlds. This combines a lot of disparate philosophical thought experiments into one major thought experiment, of which I advocate it as both good for oneself, good from both the shallow and deep green views of environmental ethics, and how it circumnavigates problems people have with mind upload and the experience machine all at once. I will also advocate that this is a realistic goal to work towards, as a political transhumanist strategy.

# The Short version

We begin with a short version of the argument as follows:

Definitions:  
ASI = Artificial Super Intelligence  
FASI = Friendly Artificial Super Intelligence

1. Entering a (modified) experience machine is good in conditions C
2. Mind-Upload is survivable in conditions D
3. FASI is possible
4. ASI can ensure conditions C and D
5. We can tell FASI to ensure C and D
6. We benefit from and The environment is better of having the environment reshaped to ensure C and D
7. It is good to tell FASI to upload us using all resources available

The scenario in general has been supported as a good future with decent probability of occurring such as for instance Nick Bostr√∂m's [book Deep Utopia](https://nickbostrom.com/deep-utopia/). Premises 1-3 and 6 seem highly controversial, 4 and 5 only partially so.

# Modified Experience machine

The original experience machine in Nozick's "[The examined life](https://en.wikipedia.org/wiki/The_Examined_Life)" is a thought experiment in which an individual is asked to enter a machine that is able to simulate any experience they'd like. To not take away any of the enjoyment that is supposedly diminished by the belief that one is in an experience machine, one's memory is deleted. One can also allow other people to enter their own experience machine to interact with them. The original argument was as primarily as a counter-intuition against the hedonic-pleasure account of self-interest.

The modified version (i.e conditions C) sets out to optimize for good for the individual. Thus it wouldn't be a counter-intuition to hedonism, for in the experience machine one could program in less hedonic states of consciousness. We can program in desires and satisfying them, we can program in objective list maximization, or some alternative/hybrid theory of self-interest.

Our modified version don't need to erase memory, as per Nozick's original experience machine. If the machine would contain memory erasure, [this might be considered death by some accounts of identity of the self](https://iep.utm.edu/experience-machine/). We don't need to erase memory for the effect to exist. It might be enough with a belief change that the experiences in the machine is just as real as the outside one, or similar philosophical views, which would prevent death by those accounts of identity of the self.

The way this can be practically achieved is with Full-Dive Virtual Reality (FDVR) which has grown in popularity in transhumanist communities and is as of yet still very niche, but will likely become a very mainstream concept fairly soon (even if with a different name). I expect all societies to shift towards pro-entering the machine, once we dip our feet into truly immersive virtual reality. This way, the modified experience machine is a feasibly future technology.

## Counter-arguments answered

A common problem for experience machine thought experiments are imaginative failures, where individuals (including professional philosophers) think there's something else to the thought experiment than presented, such as the machine failing to produce the desires experiences. The imaginative failures which occur can be remedied in this scenario sketched out, as it will provide the possibilities of how such a scenario can actually play out practically in the world.

Status quo bias may impact the willingness of individuals to enter the machine. But imagine you are already in an experience machine and it's currently playing out your current experience. Would you leave it? Would you start a new simulation? Imagine if you played your pre-entering life.

What about having the option to die? A death button just in case? In the worst case horror scenario; The experience machine malfunctions, giving awful experiences. The death button is broken. As we'll go over later, a friendly ASI should have the complete ability to prevent such an outcome.

## Moral Concerns

While the original thought experiment would exclude moral concerns as irrelevant, as this example aims at practical implementation I think it would be useful to go over consideration of ethical principles, atleast to convince people for whom prudential rationality is not the only motivating force in action-taking.

Hedonic utilitarians should accept it, for it increases pleasure for everyone, especially if people tune or if its forcibly tuned to maximize pleasure. If individuals have different preference such that some put moral worth on freedom, then atleast some negative utilitarianism can be accepted in which involuntary suffering can be removed.

From a contractarian perspective, it would be better to allow individuals to enjoy their own preferences with some social contract enforced between all agents (a contract which can be properly specified, likely in formal logic) in ways that everyone can accept and justify to other agents.

What about holding obligations to friends or otherwise value friendship? Nozick did allow for others to enter the experience machine with you, so any obligations or friendships would not be negative. So invite them on! Let's get as many as possible! However the experience would not be different if it was a real person or just some virtual world generated avatar.

What about those who value the external world? I support Chalmers view that virtual worlds are just as real as the external world, for instance, a glass of water if it was in a virtual world (or if we're in a simulated world) would just so happen to be digital, i.e we just learnt a new property of the glass of water. So we wouldn't give up any kind of external world, we'd just replace some properties (unless they already had them in the first place, such as if we're already living in a simulated world). The real world would just be different, as digital worlds are just a different kind of real world.

# Mind-Upload

When it comes to mind-upload, I do not care about which theory of personal identity over time is true or anything like that, whether that be psychological continuity or bodily continuity. I care about the continuation of a unified consciousness without splitting (constitutive panpsychism), universal merging (cosmopsychism, analytical idealism), or elimination of consciousness (most accounts of biology-centered physicalism) under the process of mind-upload into a computational substrate. I care a lot less about more abstract notions such as preserving biological bodies over time or continuous psychological change over time. I only care about how consciousness changes over time.

With this in mind, we can nevertheless assume some conditions D for which consciousness and personal identity is preserved through an change into different more efficient substrate for consciousness (whatever that may be like). We can perhaps gradually replace our [underlying substrate with digital neurons](https://www.consc.net/papers/uploading.pdf). Questions relating to mind-upload, whether it can be done quickly or slowly, can perhaps be resolved with an ASI with the ability to [experiment about conscious states](https://thephilosophyaddict.wordpress.com/2024/01/18/experimental-phenomenal-consciousness-research/) and their substrates, fusions and fissions.

The fact is that we don't even need to reject biological substrate if so needed. Brain in vats is a type of upload that can work. I think it's likely to not be a brain-structure, but more of a mix of technological and biological systems, which enables conscious experience. For instance, [Penrose's view on consciousness for instance is bio-centric](https://en.wikipedia.org/wiki/Orchestrated_objective_reduction), and it might be bad to give up microtubles collapse. But the quantum wave can collapse in other substrates than the brain. [IIT](https://iep.utm.edu/integrated-information-theory-of-consciousness/) has the implication that [transistors don't integrate information between eachother](https://www.youtube.com/watch?v=0hex5katLGk). So we'd likely want to use something else. So we might have genuine, scientific reasons to avoid getting rid of the biological. Or if we do, it must atleast be different from current transistors mechanisms to retain a unified continuous consciousness and identity of self. As such, a (post?-)techno-biological substrate with more efficient computation (computronium?) would allow for the retaining of consciousness in a more efficient configuration that is able to power virtual experiences/worlds.

# Plausibility of Friendly ASI

This section will go over premises 4-6, by showing firstly how ASI is plausible, then how it can ensure conditions C and D, and then that it's plausible for ASI to be friendly.

## ASI?

Firstly, is ASI possible? We can take an argument similar to that from [Chalmers](https://consc.net/papers/singularity.pdf).

1. There is something that is like intelligence
2. The something like intelligence is ever increasing for artificial intelligence
3. In the future, the intelligence level will surpass that of all humans

This seems like a reasonable prediction, and may only be wrong barring some serious capability limitation such as [Lukas-Penrose arguments](https://iep.utm.edu/lp-argue/) where AI's will forever be incapable of proving "intuitively true" G√∂del sentences.

We could specify an intelligence measure of course, such as in Hutter's article [can intelligence explode](https://arxiv.org/abs/1202.6177) ([my YouTube video on it](https://www.youtube.com/watch?v=-c_-cHUMOuY)) where it's defined in terms of AIXI, whereby the more intelligent being is more able to achieve its goals and compress information. The idea is that its intelligence value (which is a number from 0 to 1 in this model) may eventually surpass humans.

A similar, more common argument [is the singularity argument](https://consc.net/papers/singularity.pdf)

1. There is something that it is like to be intelligence
2. Something intelligent enough can improve its own intelligence
3. Intelligence grows exponentially

This argument provides another case for how ASI could be possible, and is perhaps slightly scarier one as it leads to a more rapid, exponential ascent into superintelligence, which may make it harder to control and align.

## Can ASI ensure conditions C and D?

Something more intelligent than a human that can be copied into a large research community seem very likely (though not necessarily so) capable of producing conditions needed for mind-upload survival and a proper experience machine without negative downsides or failures.

We can take it to be irrational if conditions C or D scares people off from the way it looks from the outside. If it involves sticking needles into the brain that sucks out like a horrid machine, we would be inclined against it compared to if it was like sitting in a golden throne as white lights and angelic music processed one into the mind-upload. The internal feeling might be the same. It's worth considering such irrationality as unwarranted, as long as we assume the ASI is friendly and that the friendly ASI has a fully developed theory of conscious experience, there's be nothing to worry about.

## Friendly ASI?

The last problem is what reasons a more intelligent community would have to serving human goals? If anything it gives us a good reason to ensure friendly AI is built, the possibilities for it, and some mild credence towards effective accelerationism as optimal strategy (given that the risk of ambivalent AI/extinction is low).

However the current state puts extinction risks from [2%](https://link.springer.com/article/10.1007/s00146-023-01748-4), [10%](https://arxiv.org/abs/2206.13353), [many AI researchers put a >10% probability on it](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/) but some are saying [99.9999%](https://www.windowscentral.com/software-apps/ai-safety-researcher-warns-theres-a-99999999-probability-ai-will-end-humanity-but-elon-musk-conservatively-dwindles-it-down-to-20-and-says-it-should-be-explored-more-despite-inevitable-doom) This is still a field of which we need to thread carefully.

The reasons things can go wrong are multiple interacting parts of reasonable assumptions with regards to already existing models of AGI such as AIXI and G√∂del machines. The list below provides reasons against this premise.

- Orthogonality thesis
- Reward misspecification
- The alignment problem
- The control problem
- The value loading problem
- Reward hacking
- Instrumental Convergence
- Goal miss-generalization
- Deceptive Alignment

More recent arguments include empirical one's

If anything, I hope this article convinced people just how powerful friendly ASI can be, and why we should do our best to make the premise true, and to minimize existential risk while balancing out the other potential failures of not building friendly ASI fast enough, generating a synthesis between the "doomers" and the "effective accelerationists". This article does however not go over the exact risks, but they are enough that we should be very careful threading forward, and I believe we should slow down AI development to ensure safety.

# Environment

Lastly, how do we gather the resources needed to create and sustain a massive complex of uploaded minds in ever more complicated virtual worlds? How do we gather the resources for conditions C and D? Well, likely by dismantling the solar system atleast as a first step. But is that ethical?

[Within environmental ethics, the most important part is about prudential rationality and goodness-for-whom](https://plato.stanford.edu/entries/ethics-environmental/). The idea of shallow green is that there are occasions when the environment is instrumentally valuable for humans. However deep-green individuals tend to counter this with the view that the environment is intrinsically valuable.

The idea I want to put forth is that my argument works for both views. For the shallow greens, the environment stops being instrumentally useful, stops being good _for humans_, so we can dismantle it. On the other hand, for the deep greens, it is good _for the environment_ to be uplifted. There's nothing that says we have to kill off the consciousness of the beings in the environment. It is good for the animals, as we can abolish all wild animal suffering. If we extend this to all natural things, including rivers, trees, rocks, earth's magma core and the rings of Saturn we end up with a whole range of things better off powering the Dyson sphere powered brain simulation or space traveling von Neumann probes. It seems incoherent to take deep green while not supporting what is in the interest of the environment, even if it involves the seeming destruction of it.

# Conclusion

The argument in the original section implies we should utilize all resources available. The extent to this would be at a minimum everything in the solar system, but likely we'd utilize everything in our future time-cone, assuming the speed of light continues to be a limit on causality.

Future ideas I have include calculating the ratio of probes vs Dyson sphere construction, aswell as a practical political project on getting more people onboard this idea.