This is a start of a project of rehabilitating logical empiricism, which will likely include more thoroughgoing explication and justification of what's said below. I believe I am justified, but I might be missing some crucial, mind-blowing counter-argument to the entire enterprise that I just haven't seen yet. A thoroughgoing rehabilitation of logical empiricism would likely also include a historical-sociological analysis of why it was/is still so widely rejected. At this point, I believe that the large rejection is more strongly based on sociological and other non-rational factors not based on argument. The structure is a point-list without entire justification, as this is more of an outline of a major project I might abandon if I find strong rational counterarguments for logical empiricism where simple modifications don't work. Therefore, I am very happy for criticism, and I am likely not correct on everything.

This post advocates for a revival of logical empiricism that defends what the logical empiricists got right against the cult of Wittgenstein, the cult of Quine and the cult of Sellars. Not to discount the philosophers themselves, but rather said followers who took their arguments against logical empiricism to an extreme. [They ruined the reputation of logical empiricism, by straw-manning](https://philpapers.org/rec/VERLPT) and, as I argue, overexaggerated the consequences of their arguments. It seems that studying the sociology and history of 1960s-1990s era shows that analytic philosophy was in grips of tarnishing the reputation of the great philosophers before them (including the Vienna circle, the Berlin circle, Kraft circle and the Lwow-Warsaw school). This is not to discount that era of philosophy and its critiques as bad, quite the opposite in fact. However, the dismissing of logical empiricism is over. I'm supporting a revival of a (albeit more moderate) logical empiricism, particularly strongly influenced by Carnap.

1. Metaphysics is mostly nonsense, unless it's a part of analyzing a specific theory. "Numbers exist" is meaningless outside of a theory but obviously true or false in some theory T based on whether it includes numbers or not. So for example, numbers obviously exist within Peano Arithmetic, but it's nonsense to say they exist "out there". The indispensability arguments make the inference from mathematical objects in our best physical theories quantum mechanics and general relativity implies that they exist "out there". But this is taking the metaphysical question outside of the theory, and it looses it's meaningfulness this way.

2. Instead of the traditional, strict version of verificationism, I will allow some metaphysical statements to still have meaning ([such as Parfit's view on personal identity](https://www.jstor.org/stable/2184309) since it includes meaningful content regarding phenomenal binding/combination laws) so long as they can be at least in principle theoretically verifiable ([Such as my own post on experimental phenomenal binding)](https://thephilosophyaddict.wordpress.com/2024/01/18/experimental-phenomenal-consciousness-research/). I do not take on a particularly strong verification criterion, universal quantification and non-complete verification is fine. In fact, as we'll see in point 10. I argue for a kind of probabilism, rather than complete verification. On that note, I also reject Popper's falsification, as then no theories (except some final theory T, or atleast one which implies all currently doable observation sentences) that could be considered true. Comparing the later Wittgenstein's use theory of meaning to the verification criterion of meaning, I do think that both have their place. Use theory has it's place in some theory of sociology S where S contains the use's of terms. But philosophers and scientists often engineer concept, and the verification criterion is good for that domain instead.

3. The analytic-synthetic distinction, well atleast 3 of them that Quine addresses, [may not exist](https://philpapers.org/rec/QUITDO-3). Instead I propose of the meaning content of a proposition consisting of a spectrum from less informative to more informative. Even a statement such as "bachelors are unmarried men" may give you some information about how the English language works (and hence not entirely analytic). This works with the previously established verification criterion, since we can verify that this is how that word is used (and increase our probabilistic belief in it). While stricter versions of the verification criterion is in threat (which makes a distinction between analytic a priori mathematical statements, synthetic a posteriori empirical statement, and meaningless metaphysical statements) our eased version does not suffer such problems.

4. The "Logical" part of logical empiricism: A set of symbols S form a language of well-formed formulas L. Together with a deductive system D they can generate a set of tautologies in a system. Theories T can then be formed as a set of premises/axioms from which one can derive theorems An example with propositional logic with the logical symbols $latex \{,\wedge,\vee,\rightarrow,\leftrightarrow,\}$, non logical symbols p,q,r,... and the logical and non-logical symbols form the well-formed formulas with a recursive grammar, and some deductive system (say, Hilbert deduction with classical logic) which contain all the inference rules such as law of excluded middle, law of non-contradiction, and modus ponens. A theory T is then a set of any propositions in L such as $latex \{p\rightarrow q, p\}$. From that we can derive, applying the deductive system's rule modus ponens, $latex \{p\rightarrow q, p\} \vdash_D q$. Now for the philosophical part: all scientific theories can be formulated in some theory T ([the syntactic view in the structure of scientific theories](https://seop.illc.uva.nl/entries/structure-scientific-theories/#SynVie), which has gained a resurgence in popularity in recent years).

5. The "Empiricism" part of logical empiricism: Following Carnap's project in the Aufbau, all scientific theories T are reducible to some logical theory and phenomenal predicates. Preferable to use simpler logical systems, such as first-order predicate logic unless something else is necessary. He then went further than I would by minimize the number of phenomenal predicates to only a single one, the "phenomenal similarity" relation $latex R$ where $latex xRy$ is true iff $latex x$ and $latex y$ are a similar phenomenal experience. In the later part of Aufbau, he tries to rid of even the phenomenal similarity predicate, but I don't think that works. Instead I propose my own system, [formal phenomenology](https://thephilosophyaddict.wordpress.com/2023/11/25/formal-phenomenology/) (or even something like [Barry Smith's formal ontology](https://philpapers.org/archive/SMITBT.pdf) when applied to phenomenal consciousness) should be used instead. Utilizing recent research into the mathematical structures of phenomenal consciousness, it's better to use many phenomenal predicates (hue, brightness, loudness, pitch, smell e.t.c). The Vienna circle did emphasize regions of visual patches, and in formal phenomenology this is formalized within set theory to give us subsets $latex v\subset V$ of the visual field to act as such patches. I still agree with Carnap however in how to in reducing theoretical predicates P to phenomenal predicates R is done via some kind of Ramsey-Lewis method and explicative definitions from conceptual engineering. All observation sentences of all theories O have it such that O is completely reducible via definitions to sets of phenomenal experience and the similarity relation R. This kind of reductionism is in stark contrast with current movements to reduce phenomenal predicates to physical predicates, which I believe is still an important project but which has limitations.  

6. Scientific theories are (Hempel-Schaffner)-reducible to each other in a mostly linear ordering. An upper theory $latex T_u$ is Hempel-Schaffner reducible to $latex T_b$ iff $latex T_b \vdash T_u$. This allows for a kind of metaphysical foundationalism (assuming theory reduction terminate at some theory $latex T_b^*$). Such a foundational theory logically implies all observation sentences $latex T_b^* \vdash O$. Also, there is most likely no downward causality. We currently don't have a final theory, so humanities currently best foundation are perhaps the 2 incompatible theories Quantum Field Theory and General Relativity. A final theory will most likely a [new theory of Quantum Gravity (and consciousness](https://en.wikipedia.org/wiki/Theory_of_everything_(philosophy))) at the bottom of the chain to unify them all into a final theory (though we can't be certain it's the final theory, especially if there's underdetermination in its metaphysical implications, in a similar way measurement in quantum mechanics is underdetermined).  

7. A commitment to scientific naturalism. Philosophy is not a foundation for science, but evolves simultaneously and symbiotically with it. Philosophy deals with the most abstract parts of scientific theories, description of theories in formal logic, and reduction relations. Philosophy also formalizes the scientific process and formalizes assumptions various theories make. Knowledge is formalized in cognitive science and artificial intelligence, likewise the philosophy of mind but with neuroscience (multiple realizability may be true though, so neuroscience is not enough) and ethics with evolutionary psychology (such as game theoretic cooperative behavior among scarce resources for bodies undergoing homeostasis, reproduction and extropy).  

8. Kuhn is mostly not correct, there is algorithmic progress in the philosophy of science. This progress is however more Lakatosian than Popperian. There are no massive paradigms with incommensurability between them, only multiple smaller research projects who are non-incommensurable and who shift in favor not for sociological reasons but for methodological reasons. Ways to formalize the algorithmic progress of science may include Hempel's various [theories of explanation](https://plato.stanford.edu/entries/scientific-explanation/#DNMode), hypothesis testing models ([formal learning theory](https://plato.stanford.edu/entries/learning-formal/)) and such. I do think that the Quine-Duhem thesis still holds, and the new problem of induction, which both allow some degree of underdetermination of scientific theories (we may still select simplest theories, perhaps using a measure such as program length as seen in algorithmic information theory). Scientific discovery can also sometimes be caused by some semi-rational process, however actually applying concepts still requires rationality. The typical example of the discovery of benzene in organic chemistry by dreaming about a [snake biting its own tail](https://en.wikipedia.org/wiki/August_Kekul%C3%A9#Kekul%C3%A9's_dream). The problem is that anyone can dream this, but fail to apply it rationally. At best, it caused an associated thought of the benzene. Application and discovery is still largely algorithmic, though it might be hard to make such an algorithm (if it was easy we'd likely already fully automated science by now).  

9. Knowledge can only be gained through the senses and no other way. Each phenomenal region P is in fact a given (and it is not a myth). This can be formalized in formal phenomenology. Phenomenal experience and memory of phenomenal experience and the similarity relation R, alternatively Barry Smith's topological formalization of Husserl's phenomenology, are what basic beliefs consist of. They are, as Sellars points out, atleast somewhat theory laden. As such we instead adopt [Susan Haack's foundherentism](https://www.wiley.com/en-us/Evidence+and+Inquiry%3A+Towards+Reconstruction+in+Epistemology-p-9780631196792) and [Thagard's formalization](https://mitpress.mit.edu/9780262700924/coherence-in-thought-and-action/) of it. This means that our theories also epistemically justify our observations, and not just having observations as basic beliefs which justify theories. This is also true for mathematical knowledge. My main argument comes from a reductionist thesis in [varieties of consciousness](https://global.oup.com/academic/product/the-varieties-of-consciousness-9780199846122?cc=se&lang=en&). Is there a thought-phenomenal field that's separate from [visual space V, auditory space A etc](https://thephilosophyaddict.wordpress.com/2023/11/25/formal-phenomenology/)? It seems like thought is just hearing audio of an inner voice or imagining visual things, with no special phenomenal space on its own. Some have argued that the "A-Hah!" feeling is separate, but it doesn't seem like it (and why would such a specific phenomenal experience be special?). Though this is from basic introspection rather than empirical science, so I might be very wrong.  

10. The analysis of knowledge is not based on listing sufficient and necessary conditions (justified, true, belief, other criterions) based on thought experiments such as red barn country and truetemp that lead nowhere and use the "method of cases", which has been in recent years put into question by [experimental philosophy to be way to dependent on non-rational factors such as ethnicity and gender](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4975.2007.00157.x). Instead we propose a belief system informed by rational agent action, utilizing the [Dutch book arguments](https://plato.stanford.edu/entries/dutch-book/) to argue for probabilistic beliefs, and the diachronic Dutch books for updating beliefs with evidence [using Bayes's formula](https://plato.stanford.edu/entries/epistemology-bayesian/). The problem of the priors is best solved with putting priors to objective probabilities (the principal principle) which justifies the convergence theorems (thus giving us a good explanation for why scientists converge on belief over time). Such convergence justifies scientific realism, a kind of optimistic meta-induction (IBE is not needed to justify realism). Such convergence of theories might in turn justify metaphysical realism (assuming a final theory contains external-world predicates, and doesn't lead to probabilistically extremely likely outcomes such as Boltzmann brains).

As a final aside, the system can be visualized as a cylinder like this:

![](https://thephilosophyaddict.files.wordpress.com/2024/04/image.png?w=744)

The arrows are logical inferences (As we can see, every theory is Hempel-Schaffner reducible to eachother, and theories derive observation sentences) and they all imply observation sentences (which can be reduced to phenomenal experiences described in formal phenomenology). Epistemic justification, however, unlike in Carnap where the arrows would just go from the different O's to the theories, they go both directions at once (which is what gives us Susan Haacks foundherentism). Every proposition and observation is a part of a coherent whole, based on Thagard's formalism.

In conclusion, this is a very ambitious project, and likely an uphill battle given the reputation logical empiricism has. The reputation may be because of rational reasons, rather than non-rational reasons. But so far it seems like a large part of said rejection is only somewhat based on rational reasons. I hope to eventually be able to see why so many find logical empiricism to be wrong, or make a breakthrough in its rehabilitation. For now, I will call myself a neo-logical-empiricist (or some more catchy name).