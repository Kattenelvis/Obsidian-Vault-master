

[(174) The orthogonality thesis & AI optimism - YouTube](https://www.youtube.com/watch?v=8H3dblxkLhY&ab_channel=LanceIndependent)



21:00 Current AI's might reach a limit, there's a vastly more compute efficient one that might kill us all. Why be skeptical of this? The human brain is vastly more compute efficient, only a few watts for computing gigabytes-terrabytes of data. Do you think this is just a hardware thing? What about brains grown in vats as something that could possible be the new paradigm? 


34:00 Final goals as the underlying objective function in their neural networks and RLHF. Underlying goal can be changed beyond just being an AI assistant via prompt hacking. Chaos-GPT for instance seems to have been prompted to have the goal of ending humanity. If we apply the intentional stance, whatever kind of goal it seems to have, we can model it as having, even if it might be really complicated in the case for humans. A social context is still a set of propositions in some possible worlds that the preference relation is defined over. 

36:00 What about language agents that use LLMs as a part? They could in principle go on indefinitely with a specified final goal, even if the final goal is specified in natural language and isn't taken completely literally like some theorized before LLMs. 

42:00 Objective functions? Utility functions? Even preference relations? For AIs? What about AI that tries to be rational and uphold the Von Neuman Morgenstern axioms of rational choice? (Omhundro 2008). 

1:00:00 I think Carnapian conceptual construction becomes very useful in these kinds of situations. Firstly, I think one can always accept the definitions of words people use. One can simply put say, a star, or a subscript etc. so for example, "intelligence_1" and "intelligence_2" as the definiandum with their respective definians (which may or may not turn out to be equivalent in the end, see the Church-Turing thesis or Evolutionary Stability in Evolutionary Game Theory where different definitions turned out to be equivalent). Regardless, I don't think a definition can ever be "wrong", and one should always be ready to just accept someone elses definition so long as one keeps in mind the implications.

Hutter's "Can intelligence explode" has a definition of intelligence which is a function from agents to the interval between 0 to 1 with AIXI as 1. The definition involves how well one can achieve any objective function in any arbitrary environment. 

With no free lunch, surely some AI models can still be strictly superior in all environments?

